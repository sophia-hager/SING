{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thFK9gwfldz1"
   },
   "source": [
    "## Import Things so the Code Runs\n",
    "(Adjust your drive path!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd3K0avvtOAX",
    "outputId": "30e134b1-d60b-45c4-b356-42a04a432638"
   },
   "outputs": [],
   "source": [
    "drive = False  # False for Local\n",
    "if drive:\n",
    "    !pip install pretty_midi\n",
    "    !pip install -U sparsemax\n",
    "# locally, also compile torch with CUDA enabled:\n",
    "# conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UhjTVBAwtYJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi\n",
    "#this package is used to write it back into music.\n",
    "from mido import Message, MidiFile, MidiTrack\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.distributions\n",
    "import sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8GxcYRNJ021i",
    "outputId": "2801439b-f924-4988-965b-33afe8ccafcb"
   },
   "outputs": [],
   "source": [
    "if drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    my_drive_path = '/content/drive/MyDrive/2022_Special_Studies_Hablutzel/ModelCopies/'\n",
    "else: # local\n",
    "    my_drive_path = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fwoOduJlbgC"
   },
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iuKD5iNkyChL"
   },
   "outputs": [],
   "source": [
    "#this is the model\n",
    "class music_generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, base_lstm=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size  # 128\n",
    "        # output_size is num expected features (128)\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers=1, bidirectional=False)\n",
    "        self.attention = nn.Linear(2, 1)\n",
    "        self.softmax = sparsemax.Sparsemax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden = None\n",
    "        self.base_lstm = base_lstm  # true to use lstm without attention\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # set hidden state to zeros after each batch\n",
    "        hidden = (torch.zeros(1, batch_size, self.hidden_size)).float() # .to(\"cuda:0\")  # [layers, batch_size, hidden_size/features]\n",
    "        self.hidden = (hidden, hidden) # hidden_state, cell_state\n",
    "        return\n",
    "\n",
    "    def set_random_hidden(self, batch_size):\n",
    "        # create new random hidden layer\n",
    "        hidden = (torch.randn(1, batch_size, self.hidden_size)).float() # .to(\"cuda:0\")\n",
    "        self.hidden = (hidden, hidden)\n",
    "        return\n",
    "\n",
    "    def forward(self, input, batch_size, prev_sequence, batched_ssm):\n",
    "        # look at tensor things - view vs. reshape vs. permute, and unsqueeze and squeeze\n",
    "        # try looking at the LSTM equations\n",
    "        # .to('cpu')  # returns a copy of the tensor in CPU memory\n",
    "        # .to('cuda:0')  # returns copy in CUDA memory, 0 indicates first GPU device\n",
    "        # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to\n",
    "\n",
    "        # sequence length\n",
    "        # size of input (10 or 1)\n",
    "        sequence_length = input.size()[0]\n",
    "\n",
    "        # Run the LSTM\n",
    "        # output - sequence of all the hidden states\n",
    "        # hidden - most recent hidden state\n",
    "        # input dimensions: [sequence_length, batch_size, 128]\n",
    "        output, self.hidden = self.lstm(input.float(), self.hidden) # removed input.tocuda, so hidden doesn't need cuda either # make sure hidden in cuda memory! - #0].to(\"cuda:0\").float(), hidden[1].to(\"cuda:0\").float()))\n",
    "        # output dimensions: [sequence_length, batch_size, 128]\n",
    "        # outputs as many beats (sequence_length) as there were beats in the input\n",
    "        # hidden: last hidden states from last beat\n",
    "\n",
    "        # Get copy of the output\n",
    "        # take out this line after finish LSTM (other than reshaping) - unsqueeze to get right reshaping\n",
    "        output_1 = output.to('cpu') # [-5:,:,:]  # don't just take last 5\n",
    "\n",
    "        #########################\n",
    "        # attention starts here #\n",
    "        #########################\n",
    "        \n",
    "        # output without attention\n",
    "        new_output = output_1.view(sequence_length, batch_size, 1, 128)  # reshape\n",
    "        avg_output = torch.sum(new_output, 2)  # return this for base LSTM (w/o attention)\n",
    "\n",
    "        # return early (w/o attention) for base lstm\n",
    "        if self.base_lstm:\n",
    "          return avg_output, self.hidden  # TODO: test return type\n",
    "        \n",
    "        #this variable holds the output after the attention has been applied.\n",
    "        seqs = []\n",
    "\n",
    "        # slice the batched ssms to the right places\n",
    "        beat_num = prev_sequence.shape[0]\n",
    "        # find the row for this beat in each ssm\n",
    "        # batched_ssm shape is (batch_size*beats, beats), bc all the pieces are stacked vertically atop each other\n",
    "        inds_across_pieces = range(beat_num, batched_ssm.shape[0], batched_ssm.shape[1])  # eg 11, 2625, 105 - indices of this beat in each of the pieces in the batched_ssm\n",
    "        # for the row for this beat in each ssm, slice the row up to (not including) this beat\n",
    "        ssm_slice = batched_ssm[inds_across_pieces, :beat_num]\n",
    "        \n",
    "        # sparsemax makes entries in the vector add to 1\n",
    "        weights = self.softmax(ssm_slice)\n",
    "        # print(\"weights shape:\", weights.shape)\n",
    "\n",
    "        # this is the sparsemaxed SSM multiplied by the entire previous sequence\n",
    "        # to scale the previous timesteps for how much attention to pay to each\n",
    "        weighted = (prev_sequence.T*weights).T # replace .T - see which dimensions we're switching\n",
    "\n",
    "        # then it's summed to provide weights for each note.\n",
    "        weight_vec = (torch.sum(weighted, axis=0))\n",
    "        #This concatenates the weights for each note with the output for that note, which is then run through the linear layer to get the final output.\n",
    "        pt2 = torch.hstack((weight_vec.unsqueeze(1), avg_output[0,:,:].unsqueeze(1))).transpose(1,2)\n",
    "        attentioned = self.attention(pt2.float()).permute(2,0,1)  # before .permute() .to(\"cuda:0\")).to('cpu')\n",
    "\n",
    "        # final added new output to seqs.\n",
    "        seqs.append(attentioned)\n",
    "\n",
    "        # seqs is combined into one tensor and returned, along with the hidden and cell states.\n",
    "        newoutput = torch.vstack(seqs)\n",
    "\n",
    "        # delete vars to remove clutter in memory\n",
    "        del seqs\n",
    "        del pt2\n",
    "        del attentioned\n",
    "        del weight_vec\n",
    "        del weighted\n",
    "        del weights\n",
    "        del ssm_slice\n",
    "        del inds_across_pieces\n",
    "        del beat_num\n",
    "        del new_output\n",
    "        del avg_output\n",
    "\n",
    "        return newoutput.double(), self.hidden  # hidden = hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ0UrEmslTP-"
   },
   "source": [
    "## For Training the Model and Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rHgQoF4-Usqv"
   },
   "outputs": [],
   "source": [
    "class model_trainer():\n",
    "  def __init__(self, generator, optimizer, data, hidden_size=128, batch_size=50):\n",
    "    self.generator = generator\n",
    "    self.optimizer = optimizer\n",
    "    self.batch_size = batch_size  # play with this\n",
    "    self.hidden_size = hidden_size  # 128\n",
    "    self.data = data\n",
    "    self.data_length = data[0][0].shape[0]  # as long as piece length doesn't vary\n",
    "\n",
    "  def train_epochs(self, num_epochs=50, full_training=False, variable_size_batches=False, save_name=\"model\"):\n",
    "    #trains each epoch\n",
    "    losslist = []\n",
    "    #useful when you want to see the progression of the SSM over time\n",
    "    piclist = []\n",
    "\n",
    "    for iter in tqdm(range(0, num_epochs)):\n",
    "      # start training the generator\n",
    "      generator.train()\n",
    "\n",
    "      if full_training and variable_size_batches:\n",
    "        # use all data, and group batches by piece size\n",
    "        batches = make_variable_size_batches(self.data)\n",
    "      elif not full_training and variable_size_batches:\n",
    "        batches = make_variable_size_batches(self.data[:100])  # buggy\n",
    "      elif full_training:\n",
    "        # use all data\n",
    "        batches = make_batches(self.data, self.batch_size, self.data_length)\n",
    "      else:\n",
    "        # use first 100 pieces\n",
    "        # can we overfit on a small dataset? if so, can be a good thing b/c shows the model can learn\n",
    "        batches = make_batches(self.data[:100], self.batch_size, self.data_length)\n",
    "\n",
    "      cum_loss = 0\n",
    "      for batch_num in tqdm(range(len(batches))):\n",
    "        batch = batches[batch_num]\n",
    "        if full_training:\n",
    "          # train on full-length pieces\n",
    "          loss = self.train(batch)\n",
    "        else:\n",
    "          # train on first 105 beats of each piece\n",
    "          loss = self.train(batch[:,:105,:])  # [batch, beats, 128]\n",
    "        cum_loss+=loss\n",
    "        del batch\n",
    "        del loss\n",
    "      del batches\n",
    "    \n",
    "      # save generator after each epoch\n",
    "      curr_file = my_drive_path + save_name + \"_\" + str(iter) + \".txt\"\n",
    "      !touch curr_file\n",
    "      torch.save(generator, curr_file)\n",
    "\n",
    "      # generate example piece for piclist\n",
    "      snap = self.generate_n_examples(n=1, length=95, starter_notes=10)\n",
    "    \n",
    "      # print loss for early stopping\n",
    "      print(cum_loss)\n",
    "\n",
    "      losslist.append(cum_loss) \n",
    "      piclist.append(snap)\n",
    "      del snap\n",
    "        \n",
    "      # early stopping:\n",
    "      # after each epoch,\n",
    "      # run w/ validation\n",
    "      # if devset (validation) loss goes up for ~5 epochs in a row, early stopping\n",
    "            \n",
    "      # if last 5 losses went up, model isn't getting better, so stop early\n",
    "      if (cum_loss > losslist[-1]) and \n",
    "    return losslist, piclist\n",
    "\n",
    "  # train for one batch\n",
    "  def train(self, batch, starter_notes=10):\n",
    "    #seed vectors for the beginning:\n",
    "    batch_size = batch.shape[0]\n",
    "    self_sim = batch_SSM(batch.transpose(0,1), batch_size)  # use variable batch size\n",
    "    sequence = batch[:,0:starter_notes,:].transpose(0,1)  # start w/ some amount of the piece - 10 might be a bit much\n",
    "    generated = batch[:,0:starter_notes,:].transpose(0,1)\n",
    "\n",
    "    # reset hidden to zeros for each batch\n",
    "    generator.init_hidden(batch_size)\n",
    "        \n",
    "    # zero the gradients before training for each batch\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    # for accumulating loss\n",
    "    loss = 0\n",
    "\n",
    "    # first .forward on sequence of num_starter_beats (~5 or 10 or so)\n",
    "    # then loop from there to generate one more element\n",
    "    next_element = sequence\n",
    "\n",
    "    # take\n",
    "    for i in range(0,batch.shape[1]-starter_notes):  # for each beat\n",
    "      # iterate through beats num_starter_beats-400, generating for each piece in the batch as you go\n",
    "      val = torch.rand(1)  # probability it uses original - teacher forcing\n",
    "\n",
    "      # generate a beat for each piece in the batch\n",
    "      # we need to do this even in cases of teacher forcing, so we can calculate loss\n",
    "      output, _ = self.generator.forward(next_element, batch_size, sequence, self_sim)  # returns output, hidden - we don't need the latest copy of hidden\n",
    "      \n",
    "      if (val > .8):\n",
    "        # teacher forcing - 20% of the time,  use original from piece instead of output\n",
    "        next_element = batch[:,i+1,:].unsqueeze(0)  # [1, 0/deleted, 128] to [1, 1, 128]\n",
    "      else:\n",
    "        # 80% of the time we keep the output\n",
    "        # take last output for each batch\n",
    "        next_element = topk_batch_sample(output, 1)\n",
    "      \n",
    "      # add next_element (either generated or teacher) to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\"))) # .unsqueeze(0)\n",
    "      # append output (generated - not teacher forced) for loss\n",
    "      generated = torch.vstack((generated, output))  # used for loss\n",
    "    \n",
    "    # run loss after training on whole length of the pieces in the batches\n",
    "    single_loss = custom_loss(generated[starter_notes:,:,:], batch.transpose(0,1)[starter_notes:,:,:])\n",
    "    single_loss.backward()\n",
    "\n",
    "    # update the parameters of the LSTM after running on full batch\n",
    "    self.optimizer.step()\n",
    "\n",
    "    loss += single_loss.detach().to('cpu')\n",
    "    del next_element\n",
    "    del self_sim\n",
    "    del sequence\n",
    "    del generated\n",
    "    del single_loss\n",
    "    return (loss)\n",
    "\n",
    "  def generate_n_pieces(self, initial_vectors, n_pieces, length, batched_ssm):\n",
    "    # generates a batch of n new pieces of music\n",
    "\n",
    "    # freeze generator so it doesn't train anymore\n",
    "    self.generator.eval()  \n",
    "    # start generator on random hidden states and cell states\n",
    "    self.generator.set_random_hidden(n_pieces)\n",
    "  \n",
    "    # initial vectors in format [batch_size, num_notes=10, 128]\n",
    "    # change sequence to [10, batch_size, 128]\n",
    "    sequence = initial_vectors.transpose(0,1)\n",
    "    \n",
    "    # generate [length] more beats for the piece\n",
    "    for i in range(0, length):  # one at a time\n",
    "      with torch.no_grad():\n",
    "        # use n_pieces to generate as the batch size\n",
    "        output, _ = self.generator.forward(sequence.float(), n_pieces, sequence, batched_ssm)\n",
    "        next_element = topk_batch_sample(output, 1)\n",
    "      # add element to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\")))\n",
    "\n",
    "    # return sequence of beats\n",
    "    return sequence\n",
    "  \n",
    "  def generate_n_examples(self, n=1, length=390, starter_notes=10, source_piece=0):\n",
    "    # get piece from the data\n",
    "    piece = self.data[source_piece][0].unsqueeze(0)  # in format [1, 400, 128]\n",
    "\n",
    "    # take first 10 notes in format [1, 10, 128]\n",
    "    first_vec = piece[:,0:starter_notes,:]\n",
    "\n",
    "    # create batched SSMs for each piece\n",
    "    batched_ssms = batch_SSM(piece.transpose(0,1), n)\n",
    "\n",
    "    # generate pieces\n",
    "    new_gen = self.generate_n_pieces(first_vec, n, length, batched_ssms)\n",
    "\n",
    "    # clean up variables\n",
    "    del piece\n",
    "    del first_vec\n",
    "    del batched_ssms\n",
    "\n",
    "    # return pieces\n",
    "    return new_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PJkbSoPlQV0"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QghsXxPCngY8"
   },
   "outputs": [],
   "source": [
    "#this function takes in the piece of music and returns the chroma vectors\n",
    "def get_chroma(roll, length):\n",
    "    chroma_matrix = torch.zeros((roll.size()[0],12))\n",
    "    for note in range(0, 12):\n",
    "        chroma_matrix[:, note] = torch.sum(roll[:, note::12], axis=1)\n",
    "    return chroma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kbVFABahqRyr"
   },
   "outputs": [],
   "source": [
    "#this takes in the sequence and creates a self-similarity matrix (it calls chroma function inside)\n",
    "def SSM(sequence):\n",
    "  #tensor will be in form length, hidden_size (128)\n",
    "  cos = nn.CosineSimilarity(dim=1)\n",
    "  chrom = get_chroma(sequence, sequence.size()[0])\n",
    "  len = chrom.size()[0]\n",
    "  SSM=torch.zeros((len, len))\n",
    "  for i in range(0, len):\n",
    "    SSM[i] = cos(chrom[i].view(1, -1),chrom)\n",
    "  return (SSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TL98RMlOPkFA"
   },
   "outputs": [],
   "source": [
    "#this bundles the SSM function.\n",
    "def batch_SSM(seq, batch_size):\n",
    "  # takes sequence in format\n",
    "  # [beats=400, batch_size, 128]\n",
    "  # print(\"SSM\\tsequence_shape\", seq.shape)\n",
    "  SSMs = []\n",
    "  for i in range(0, batch_size):\n",
    "    # print(\"SSM\\tsequence\", seq[:,i,:].shape)\n",
    "    ssm = SSM(seq[:,i,:])  # [beats, batch, 128]\n",
    "    # print(\"SSM\\tssm\", ssm.shape)\n",
    "    SSMs.append(ssm)  \n",
    "  return torch.vstack(SSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zWRL4AHyhqJo"
   },
   "outputs": [],
   "source": [
    "# Takes in the batch size and data and returns batches of the batch size\n",
    "def make_batches(data, batch_size, piece_size=800):\n",
    "  random.shuffle(data)\n",
    "  batches = []\n",
    "  if batch_size > 1:  # make batches\n",
    "    num_batches = len(data)//batch_size\n",
    "    for i in range(0, num_batches):\n",
    "      batch = torch.cat(list(np.array(data)[i*batch_size: (i+1)*(batch_size)][:, 0])).view(batch_size, piece_size, 128)\n",
    "      batches.append(batch)\n",
    "  else:  # each piece is its own batch - doesn't use passed-in piece_size\n",
    "    for i in range(len(data)):\n",
    "      # removes tempo info from data, but leaves 1 piece per batch\n",
    "      piece_size = data[i][0].shape[0]\n",
    "      batch = data[i][0].view(1, piece_size, 128)\n",
    "      batches.append(batch)\n",
    "      # print(batches[i])\n",
    "  # print(batches)\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uB299qQY8xSQ"
   },
   "outputs": [],
   "source": [
    "# returns batches where piece size is constant within the batch\n",
    "# but piece size is different across batches\n",
    "# and batches are in random order\n",
    "def make_variable_size_batches(data, min_batch_size=10):\n",
    "  # sort data by num beats (element at index 2 in each sublist)\n",
    "  data.sort(key = lambda x: x[2], reverse=False)  # sort descending\n",
    "\n",
    "  # split data into batches, where each batch contains pieces of the same size\n",
    "  batches = []\n",
    "\n",
    "  i = 0  # counter of pieces\n",
    "  \n",
    "  while i < len(data):\n",
    "    this_batch = []\n",
    "    pieces_this_batch = 0\n",
    "    current_beats = data[i][2] # num beats in this batch\n",
    "\n",
    "    # for all pieces with this # of beats\n",
    "    while i < len(data) and data[i][2] == current_beats:\n",
    "      # get tensor from row of data, and reshape \n",
    "      just_tensor = data[i][0].view(1, data[i][0].shape[0], 128)  \n",
    "      this_batch.append(just_tensor)\n",
    "\n",
    "      # increment counters\n",
    "      i += 1\n",
    "      pieces_this_batch += 1\n",
    "\n",
    "    # print(\"this batch\", this_batch)\n",
    "    # print(\"shapes in batch\")\n",
    "    # for p in this_batch:\n",
    "      # print(\"\\t\", p.shape)\n",
    "        \n",
    "    # only save large enough batches\n",
    "    if pieces_this_batch >= min_batch_size:\n",
    "        # reformat pieces in this batch into one tensor of size [batch size, beats, 128]\n",
    "        batch = torch.cat(this_batch, dim=0)\n",
    "\n",
    "        # store batch\n",
    "        batches.append(batch)\n",
    "\n",
    "    # clean up variables\n",
    "    del this_batch\n",
    "    del pieces_this_batch\n",
    "    del current_beats\n",
    "\n",
    "  # randomize batches order\n",
    "  random.shuffle(batches)\n",
    "\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w0luDdramUyB"
   },
   "outputs": [],
   "source": [
    "#sampling function \n",
    "def topk_sample_one(sequence, k):\n",
    "  #takes in size sequence length, batch size, values\n",
    "  softmax = sparsemax.Sparsemax(dim=2)\n",
    "  vals, indices = torch.topk(sequence[:, :, 20:108],k)\n",
    "  indices+=20\n",
    "  seq = torch.distributions.Categorical(softmax(vals.float()))\n",
    "  samples = seq.sample()\n",
    "  onehot = F.one_hot(torch.gather(indices, -1, samples.unsqueeze(-1)), num_classes = sequence.shape[2]).squeeze(dim=2)\n",
    "  return(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "86DYqbCtrpqs"
   },
   "outputs": [],
   "source": [
    "#samples multiple times for the time-step\n",
    "def topk_batch_sample(sequence, k):\n",
    "  for i in range(0, 3):\n",
    "    new= topk_sample_one(sequence, k)\n",
    "    if i ==0:\n",
    "      sum = new\n",
    "    else:\n",
    "      sum+=new\n",
    "  return(torch.where(sum>0, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hX5cldo5GTM0"
   },
   "outputs": [],
   "source": [
    "def custom_loss(output, target):\n",
    "  #custom loss function\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  weighted_mse = criterion(output.double(), target.double())\n",
    "  batch_size = output.size()[1]\n",
    "  ssm_err = 0\n",
    "  for i in range(0, batch_size):\n",
    "    SSM1 = SSM(output[:,i,:])\n",
    "    SSM2 = SSM(target[:,i,:])\n",
    "    ssm_err += (torch.sum((SSM1-SSM2)**2)/(SSM2.size(0)**2))\n",
    "\n",
    "\n",
    "  return torch.sum(weighted_mse)+ssm_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OjWY1ubk1PX"
   },
   "source": [
    "## Train Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1ie7zGkh0sO",
    "outputId": "0f4b9067-3991-416e-d33b-99fd658b5b8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d1fe15a2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data; in these small-scale tests I usually loaded the val data because it didn't take as long to load. When you are training, load the train data.\n",
    "data = torch.load(my_drive_path + \"usable_data/train_tempo_all_w_beats_30.csv\")  # make train for real training\n",
    "# data = torch.load(my_drive_path + \"usable_data/train_tempo_800.csv\")  # make train for real training\n",
    "# val = torch.load(my_drive_path + \"usable_data/validation_tempo.csv\")\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ixAE1Pyg9kL4"
   },
   "outputs": [],
   "source": [
    "# batches = make_variable_size_batches(data)\n",
    "\n",
    "# check batch sizes\n",
    "# sizes = []\n",
    "# for batch in batches:\n",
    "  # sizes.append(batch.shape[0])\n",
    "\n",
    "# sizes.sort()\n",
    "# print(sizes)\n",
    "# len(sizes)\n",
    "\n",
    "# rounding by 10: 379 batches\n",
    "# rounding by 20: ? batches\n",
    "# rounding by 30: 186 batches   # with at most 25 pieces in a batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xUfiEEqwqrjQ"
   },
   "outputs": [],
   "source": [
    "# # create model and optimizer\n",
    "generator = music_generator(128,128) # .to(\"cuda:0\")  # try without cuda\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=0.005)\n",
    "\n",
    "# parameters\n",
    "hidden_size = 128  # maybe don't touch?\n",
    "# batch_size = 10     # set to 1 for variable-size pieces\n",
    "# target_size = 800  # make this not global?\n",
    "\n",
    "# model trainer\n",
    "trainer = model_trainer(generator, optimizer, data, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-d_UFqxe0HGE",
    "outputId": "66e1fd4e-ca68-49fb-c65d-ce0fa55c8943"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]\u001b[AC:\\Users\\tinkerlab\\AppData\\Local\\Temp\\ipykernel_2036\\4033420818.py:79: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  weighted = (prev_sequence.T*weights).T # replace .T - see which dimensions we're switching\n",
      "\n",
      "  2%|██                                                                                 | 1/40 [00:29<19:01, 29.27s/it]\u001b[A\n",
      "  5%|████▏                                                                              | 2/40 [02:44<58:00, 91.59s/it]\u001b[A\n",
      "  8%|██████                                                                          | 3/40 [05:02<1:09:29, 112.69s/it]\u001b[A\n",
      " 10%|████████▎                                                                          | 4/40 [05:35<48:49, 81.37s/it]\u001b[A\n",
      " 12%|██████████▍                                                                        | 5/40 [06:50<46:06, 79.04s/it]\u001b[A\n",
      " 15%|████████████▍                                                                      | 6/40 [08:16<46:02, 81.24s/it]\u001b[A\n",
      " 18%|██████████████▌                                                                    | 7/40 [08:57<37:30, 68.21s/it]\u001b[A\n",
      " 20%|████████████████▍                                                                 | 8/40 [12:15<58:25, 109.54s/it]\u001b[A\n",
      " 22%|██████████████████▋                                                                | 9/40 [12:44<43:35, 84.37s/it]\u001b[A\n",
      " 25%|███████████████████▊                                                           | 10/40 [16:19<1:02:20, 124.68s/it]\u001b[A\n",
      " 28%|██████████████████████▌                                                           | 11/40 [16:47<45:59, 95.14s/it]\u001b[A\n",
      " 30%|████████████████████████▌                                                         | 12/40 [16:59<32:33, 69.77s/it]\u001b[A\n",
      " 32%|██████████████████████████▎                                                      | 13/40 [21:07<55:45, 123.90s/it]\u001b[A\n",
      " 35%|████████████████████████████▎                                                    | 14/40 [22:50<50:58, 117.65s/it]\u001b[A\n",
      " 38%|██████████████████████████████▊                                                   | 15/40 [22:58<35:07, 84.30s/it]\u001b[A\n",
      " 40%|████████████████████████████████▊                                                 | 16/40 [24:57<37:54, 94.79s/it]\u001b[A\n",
      " 42%|██████████████████████████████████▍                                              | 17/40 [27:53<45:44, 119.34s/it]\u001b[A\n",
      " 45%|███████████████████████████████████▌                                           | 18/40 [33:01<1:04:33, 176.06s/it]\u001b[A\n",
      " 48%|██████████████████████████████████████▍                                          | 19/40 [33:07<43:42, 124.88s/it]\u001b[A\n",
      " 50%|████████████████████████████████████████▌                                        | 20/40 [35:09<41:20, 124.03s/it]\u001b[A\n",
      " 52%|██████████████████████████████████████████▌                                      | 21/40 [36:10<33:18, 105.17s/it]\u001b[A\n",
      " 55%|█████████████████████████████████████████████                                     | 22/40 [36:53<25:58, 86.56s/it]\u001b[A\n",
      " 57%|███████████████████████████████████████████████▏                                  | 23/40 [37:03<18:00, 63.59s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▏                                | 24/40 [38:19<17:54, 67.16s/it]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████▎                              | 25/40 [38:43<13:33, 54.25s/it]\u001b[A\n",
      " 65%|█████████████████████████████████████████████████████▎                            | 26/40 [42:06<23:03, 98.79s/it]\u001b[A\n",
      " 68%|██████████████████████████████████████████████████████▋                          | 27/40 [46:52<33:37, 155.20s/it]\u001b[A\n",
      " 70%|████████████████████████████████████████████████████████▋                        | 28/40 [48:46<28:31, 142.59s/it]\u001b[A\n",
      " 72%|██████████████████████████████████████████████████████████▋                      | 29/40 [49:51<21:55, 119.57s/it]\u001b[A\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 30/40 [50:26<15:40, 94.05s/it]\u001b[A\n",
      " 78%|███████████████████████████████████████████████████████████████▌                  | 31/40 [51:44<13:22, 89.14s/it]\u001b[A\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 32/40 [53:16<12:00, 90.01s/it]\u001b[A\n",
      " 82%|██████████████████████████████████████████████████████████████████▊              | 33/40 [57:37<16:29, 141.29s/it]\u001b[A\n",
      " 85%|████████████████████████████████████████████████████████████████████▊            | 34/40 [58:02<10:39, 106.61s/it]\u001b[A\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 35/40 [58:56<07:34, 90.81s/it]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 36/40 [59:42<05:08, 77.22s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▊      | 37/40 [59:51<02:50, 56.79s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████████████████████████████████████████    | 38/40 [1:01:12<02:08, 64.20s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████  | 39/40 [1:03:22<01:23, 83.95s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [1:04:17<00:00, 96.43s/it]\u001b[A\n",
      "'touch' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "  3%|██▌                                                                         | 1/30 [1:04:17<31:04:40, 3857.96s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|██                                                                                 | 1/40 [00:21<13:44, 21.13s/it]\u001b[A\n",
      "  5%|████▏                                                                              | 2/40 [02:19<49:27, 78.08s/it]\u001b[A\n",
      "  8%|██████                                                                          | 3/40 [06:25<1:35:33, 154.96s/it]\u001b[A\n",
      " 10%|████████                                                                        | 4/40 [07:01<1:04:51, 108.10s/it]\u001b[A\n",
      " 12%|██████████                                                                      | 5/40 [11:21<1:35:00, 162.86s/it]\u001b[A\n",
      " 15%|████████████                                                                    | 6/40 [12:23<1:12:48, 128.49s/it]\u001b[A\n",
      " 18%|██████████████                                                                  | 7/40 [17:26<1:42:07, 185.68s/it]\u001b[A\n",
      " 20%|████████████████                                                                | 8/40 [18:16<1:15:56, 142.38s/it]\u001b[A\n",
      " 22%|██████████████████▍                                                               | 9/40 [18:23<51:43, 100.12s/it]\u001b[A\n",
      " 25%|████████████████████▎                                                            | 10/40 [20:28<53:51, 107.71s/it]\u001b[A\n",
      " 28%|█████████████████████▋                                                         | 11/40 [23:58<1:07:10, 138.98s/it]\u001b[A\n",
      " 30%|████████████████████████▎                                                        | 12/40 [24:10<46:50, 100.37s/it]\u001b[A\n",
      " 32%|██████████████████████████▎                                                      | 13/40 [27:26<58:11, 129.33s/it]\u001b[A\n",
      " 35%|████████████████████████████▎                                                    | 14/40 [28:10<44:53, 103.61s/it]\u001b[A\n",
      " 38%|██████████████████████████████▍                                                  | 15/40 [30:02<44:10, 106.03s/it]\u001b[A\n",
      " 40%|████████████████████████████████▊                                                 | 16/40 [30:33<33:27, 83.66s/it]\u001b[A\n",
      " 42%|██████████████████████████████████▊                                               | 17/40 [31:27<28:35, 74.58s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▉                                             | 18/40 [32:23<25:17, 68.97s/it]\u001b[A\n",
      " 48%|██████████████████████████████████████▉                                           | 19/40 [33:12<22:01, 62.93s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████                                         | 20/40 [34:22<21:41, 65.07s/it]\u001b[A\n",
      " 52%|███████████████████████████████████████████                                       | 21/40 [35:52<23:01, 72.70s/it]\u001b[A\n",
      " 55%|████████████████████████████████████████████▌                                    | 22/40 [38:37<30:05, 100.31s/it]\u001b[A\n",
      " 57%|██████████████████████████████████████████████▌                                  | 23/40 [40:31<29:37, 104.53s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▏                                | 24/40 [41:10<22:35, 84.72s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "# 28s for 1 epoch with size 100 pieces, batch size 1, full_training = False\n",
    "# full training - crashed after using all available RAM!\n",
    "##### fixed? but now going to take 3+ hours per epoch\n",
    "\n",
    "# with variable-size batches:\n",
    "# was going to take 7 hours an epoch on the lab computer (186-ish batches)\n",
    "# min batch size 5 was 76 batches, ~2 hours an epoch\n",
    "# min batch size 10 is 40 batches, ~1 hour an epoch? 30 min an epoch? much shorter\n",
    "\n",
    "# changes batches to let piece size vary\n",
    "losslist, piclist = trainer.train_epochs(num_epochs=30, full_training=True, variable_size_batches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code can save a model if you need it\n",
    "!touch my_drive_path+\"model_final.txt\"\n",
    "torch.save(generator, my_drive_path + \"model_final.txt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM model and optimizer\n",
    "generator_lstm = music_generator(128,128, base_lstm=True) # .to(\"cuda:0\")  # try without cuda\n",
    "optimizer_lstm = torch.optim.Adam(generator.parameters(), lr=0.005)\n",
    "\n",
    "# parameters\n",
    "hidden_size = 128  # maybe don't touch?\n",
    "# batch_size = 10     # set to 1 for variable-size pieces\n",
    "# target_size = 800  # make this not global?\n",
    "\n",
    "# model trainer\n",
    "trainer_lstm = model_trainer(generator, optimizer, data, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LSTM model\n",
    "losslist_lstm, piclist_lstm = trainer_lstm.train_epochs(num_epochs=30, full_training=True, variable_size_batches=True, save_name=\"model_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4KywdaS-83b"
   },
   "outputs": [],
   "source": [
    "#this code can save a model if you need it\n",
    "!touch my_drive_path+\"model_lstm_final.txt\"\n",
    "torch.save(generator_lstm, my_drive_path + \"model_lstm_final.txt\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efPcLZTbk-Zz"
   },
   "source": [
    "## View Results and Generate Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HcKawGKyoY-"
   },
   "outputs": [],
   "source": [
    "# view snapshots of pieces at each epoch\n",
    "plt.imshow(SSM(piclist[-1].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr0lTbeGSPW2"
   },
   "outputs": [],
   "source": [
    "# show loss at each epoch\n",
    "plt.scatter(range(0,len(losslist)), losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJSNH2jRPw0V"
   },
   "outputs": [],
   "source": [
    "# create example\n",
    "index = 0  # which source piece\n",
    "new_gen = trainer.generate_n_examples(n=1, length=390, starter_notes=10, source_piece=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxhjd4ijzH6-"
   },
   "outputs": [],
   "source": [
    "# show SSM for first piece\n",
    "plt.imshow(SSM(data[index][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBBYEaOAPHj_"
   },
   "outputs": [],
   "source": [
    "# show SSM for new piece\n",
    "plt.imshow(SSM(new_gen.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luEz48jIyY1U"
   },
   "outputs": [],
   "source": [
    "# show new piece\n",
    "plt.imshow(new_gen.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1Rz8WF2lE5G"
   },
   "source": [
    "## Save Example Sequence to Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jemNprz1mB2"
   },
   "outputs": [],
   "source": [
    "#this code translates the generated sequence to audio\n",
    "def stop_note(note, time):\n",
    "    return Message('note_off', note = note,\n",
    "                   velocity = 0, time = time)\n",
    "\n",
    "def start_note(note, time):\n",
    "    return Message('note_on', note = note,\n",
    "                   velocity = 120, time = time)\n",
    "\n",
    "def roll_to_track(roll, tempo):\n",
    "    delta = 0\n",
    "\n",
    "    \n",
    "    # MIDI note for first column.\n",
    "    midi_base = 0\n",
    "    notes = [0] * len(roll[0])\n",
    "    for row in roll:\n",
    "        for i, col in enumerate(row):\n",
    "            note = i\n",
    "            if col>notes[i] and col!=0: \n",
    "                if notes[i]!=0:\n",
    "                    yield stop_note(note, delta)\n",
    "                    delta = 0\n",
    "                yield start_note(i, delta)\n",
    "                delta = 0\n",
    "                notes[i] = note\n",
    "            elif col == 0:\n",
    "                if notes[i]!=0:\n",
    "                    # Stop the ringing note\n",
    "                    yield stop_note(note, delta)\n",
    "                    delta = 0\n",
    "                notes[i] = 0\n",
    "        # ms per row\n",
    "        delta += int(np.round((1/(tempo/60))*1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMlI54ma1nUt"
   },
   "outputs": [],
   "source": [
    "new_roll_final = np.vstack((new_gen.squeeze(), np.zeros(128)))\n",
    "midi = MidiFile(type = 1)\n",
    "midi.tracks.append(MidiTrack(roll_to_track(new_roll_final, data[index][1])))\n",
    "midi.save(my_drive_path + 'audio_outputs/nov21.midi')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
