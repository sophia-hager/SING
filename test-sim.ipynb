{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21571,"status":"ok","timestamp":1680108610190,"user":{"displayName":"Sophia Hager","userId":"12936426314781863045"},"user_tz":240},"id":"a98c2QVuxyw4","outputId":"7ff728aa-8b14-4610-b191-16ac5f8c2786"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pretty_midi\n","  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from pretty_midi) (1.22.4)\n","Collecting mido>=1.1.16\n","  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from pretty_midi) (1.16.0)\n","Building wheels for collected packages: pretty_midi\n","  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592303 sha256=bd908c764146dc37e54426af98ba05092fa9f7e7d7d6595923277b283eaac8ec\n","  Stored in directory: /root/.cache/pip/wheels/75/ec/20/b8e937a5bcf1de547ea5ce465db7de7f6761e15e6f0a01e25f\n","Successfully built pretty_midi\n","Installing collected packages: mido, pretty_midi\n","Successfully installed mido-1.2.10 pretty_midi-0.2.10\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sparsemax\n","  Downloading sparsemax-0.1.9-py2.py3-none-any.whl (5.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from sparsemax) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->sparsemax) (4.5.0)\n","Installing collected packages: sparsemax\n","Successfully installed sparsemax-0.1.9\n"]}],"source":["drive = True  # False for Local\n","if drive:\n","    !pip install pretty_midi\n","    !pip install -U sparsemax\n","# locally, also compile torch with CUDA enabled:\n","# conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2930,"status":"ok","timestamp":1680108613116,"user":{"displayName":"Sophia Hager","userId":"12936426314781863045"},"user_tz":240},"id":"6v6vikhkx5Ed"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pretty_midi\n","#this package is used to write it back into music.\n","from mido import Message, MidiFile, MidiTrack\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from tqdm import tqdm\n","import random\n","import torch.distributions\n","import sparsemax"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18502,"status":"ok","timestamp":1680108631610,"user":{"displayName":"Sophia Hager","userId":"12936426314781863045"},"user_tz":240},"id":"6IT3ouILx8nS","outputId":"5c0dcd1f-4031-4720-af0f-a458881cd344"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["if drive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    my_drive_path = #DRIVE PATH HERE\n","else: # local\n","    my_drive_path = \"./\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2322,"status":"ok","timestamp":1680108633927,"user":{"displayName":"Sophia Hager","userId":"12936426314781863045"},"user_tz":240},"id":"yMl4j67PyTW_","outputId":"839cc1ce-723f-45b8-8ca4-5bab373c639d"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fbd51fbc430>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# load the data; in these small-scale tests I usually loaded the val data because it didn't take as long to load. When you are training, load the train data.\n","data = torch.load(my_drive_path + \"usable_data/mar-1-variable_bin_bounds_test.csv\")  # truncate rather than padding w/ silence\n","# data = torch.load(my_drive_path + \"usable_data/train_tempo_all_w_beats_30.csv\")  # pad w/ silence at the end\n","# data = torch.load(my_drive_path + \"usable_data/validationn_tempo_round_down_30.csv\")  # load val data for small-scale tests\n","#data_train = torch.load(my_drive_path + \"usable_data/mar-1-variable_bin_bounds_train.csv\")\n","torch.manual_seed(2022)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680108633928,"user":{"displayName":"Sophia Hager","userId":"12936426314781863045"},"user_tz":240},"id":"w-1OqpZMRl84","outputId":"c79fc3ef-0637-435b-fc95-49feb072fabb"},"outputs":[{"name":"stdout","output_type":"stream","text":["438\n"]}],"source":["print(len(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_vVZAQXyt1a"},"outputs":[],"source":["#this function takes in the piece of music and returns the chroma vectors\n","def get_chroma(roll, length):\n","    chroma_matrix = torch.zeros((roll.size()[0],12))\n","    for note in range(0, 12):\n","        chroma_matrix[:, note] = torch.sum(roll[:, note::12], axis=1)\n","    return chroma_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKybTZtjyurE"},"outputs":[],"source":["#this takes in the sequence and creates a self-similarity matrix (it calls chroma function inside)\n","def SSM(sequence):\n","  #tensor will be in form length, hidden_size (128)\n","  cos = nn.CosineSimilarity(dim=1)\n","  chrom = get_chroma(sequence, sequence.size()[0])\n","  len = chrom.size()[0]\n","  SSM=torch.zeros((len, len))\n","  for i in range(0, len):\n","    SSM[i] = cos(chrom[i].view(1, -1),chrom)\n","  return (SSM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hi9GrXtqyxfZ"},"outputs":[],"source":["#this bundles the SSM function.\n","def batch_SSM(seq, batch_size):\n","  # takes sequence in format\n","  # [beats=400, batch_size, 128]\n","  # print(\"SSM\\tsequence_shape\", seq.shape)\n","  SSMs = []\n","  for i in range(0, batch_size):\n","    # print(\"SSM\\tsequence\", seq[:,i,:].shape)\n","    ssm = SSM(seq[:,i,:])  # [beats, batch, 128]\n","    # print(\"SSM\\tssm\", ssm.shape)\n","    SSMs.append(ssm)  \n","  return torch.vstack(SSMs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFAdH4Jay0OB"},"outputs":[],"source":["# Takes in the batch size and data and returns batches of the batch size\n","def make_batches(data, batch_size, piece_size=800):\n","  random.shuffle(data)\n","  batches = []\n","  if batch_size > 1:  # make batches\n","    num_batches = len(data)//batch_size\n","    for i in range(0, num_batches):\n","      batch = torch.cat(list(np.array(data)[i*batch_size: (i+1)*(batch_size)][:, 0])).view(batch_size, piece_size, 128)\n","      batches.append(batch)\n","  else:  # each piece is its own batch - doesn't use passed-in piece_size\n","    for i in range(len(data)):\n","      # removes tempo info from data, but leaves 1 piece per batch\n","      piece_size = data[i][0].shape[0]\n","      batch = data[i][0].view(1, piece_size, 128)\n","      batches.append(batch)\n","      # print(batches[i])\n","  # print(batches)\n","  return batches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKPMPo_Sy2Ty"},"outputs":[],"source":["# returns batches where piece size is constant within the batch\n","# but piece size is different across batches\n","# and batches are in random order\n","def make_variable_size_batches(data, min_batch_size=10):\n","  # sort data by num beats (element at index 2 in each sublist)\n","  data.sort(key = lambda x: x[2], reverse=False)  # sort descending\n","\n","  # split data into batches, where each batch contains pieces of the same size\n","  batches = []\n","\n","  i = 0  # counter of pieces\n","  \n","  while i < len(data):\n","    this_batch = []\n","    pieces_this_batch = 0\n","    current_beats = data[i][2] # num beats in this batch\n","\n","    # for all pieces with this # of beats\n","    while i < len(data) and data[i][2] == current_beats:\n","      # get tensor from row of data, and reshape \n","      just_tensor = data[i][0].view(1, data[i][0].shape[0], 128)  \n","      this_batch.append(just_tensor)\n","\n","      # increment counters\n","      i += 1\n","      pieces_this_batch += 1\n","\n","    # print(\"this batch\", this_batch)\n","    # print(\"shapes in batch\")\n","    # for p in this_batch:\n","      # print(\"\\t\", p.shape)\n","        \n","    # only save large enough batches\n","    if pieces_this_batch >= min_batch_size:\n","        # reformat pieces in this batch into one tensor of size [batch size, beats, 128]\n","        batch = torch.cat(this_batch, dim=0)\n","\n","        # store batch\n","        batches.append(batch)\n","\n","    # clean up variables\n","    del this_batch\n","    del pieces_this_batch\n","    del current_beats\n","\n","  # randomize batches order\n","  random.shuffle(batches)\n","\n","  return batches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3xeVBAUy6iy"},"outputs":[],"source":["#sampling function \n","def topk_sample_one(sequence, k):\n","  #takes in size sequence length, batch size, values\n","  softmax = sparsemax.Sparsemax(dim=2)\n","  vals, indices = torch.topk(sequence[:, :, 30:98],k)\n","  indices+=30\n","  seq = torch.distributions.Categorical(softmax(vals.float()))\n","  samples = seq.sample()\n","  onehot = F.one_hot(torch.gather(indices, -1, samples.unsqueeze(-1)), num_classes = sequence.shape[2]).squeeze(dim=2)\n","  return(onehot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvatBSlEy9VH"},"outputs":[],"source":["#samples multiple times for the time-step\n","def topk_batch_sample(sequence, k):\n","  for i in range(0, 3):\n","    new= topk_sample_one(sequence, k)\n","    if i ==0:\n","      sum = new\n","    else:\n","      sum+=new\n","  return(torch.where(sum>0, 1, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YI_t76XpzAfn"},"outputs":[],"source":["def custom_loss(output, target):\n","  #custom loss function\n","  criterion = nn.BCEWithLogitsLoss()\n","  weighted_mse = criterion(output.double(), target.double())\n","  batch_size = output.size()[1]\n","  ssm_err = 0\n","  for i in range(0, batch_size):\n","    SSM1 = SSM(output[:,i,:])\n","    SSM2 = SSM(target[:,i,:])\n","    ssm_err += (torch.sum((SSM1-SSM2)**2)/(SSM2.size(0)**2))\n","\n","\n","  return torch.sum(weighted_mse)+ssm_err"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wLQBwv9zC2z"},"outputs":[],"source":["# this is the model\n","class music_generator(nn.Module):\n","    def __init__(self, hidden_size, output_size, base_lstm=False):\n","        super().__init__()\n","        self.hidden_size = hidden_size  # 128\n","        # output_size is num expected features (128)\n","        self.lstm = nn.LSTM(output_size, hidden_size, num_layers=1, bidirectional=False)\n","        self.attention = nn.Linear(2, 1)\n","        self.softmax = sparsemax.Sparsemax(dim=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.hidden = None\n","        self.base_lstm = base_lstm  # true to use lstm without attention\n","\n","    def init_hidden(self, batch_size):\n","        # set hidden state to zeros after each batch\n","        hidden = (torch.zeros(1, batch_size, self.hidden_size)).float().to(device)  # [layers, batch_size, hidden_size/features]\n","    \n","        self.hidden = (hidden, hidden) # hidden_state, cell_state\n","        return\n","\n","    def set_random_hidden(self, batch_size):\n","        # create new random hidden layer\n","        hidden = (torch.randn(1, batch_size, self.hidden_size)).float().to(device)\n","        self.hidden = (hidden, hidden)\n","        return\n","\n","    def forward(self, in_put, batch_size, prev_sequence, batched_ssm):\n","        # look at tensor things - view vs. reshape vs. permute, and unsqueeze and squeeze\n","        # try looking at the LSTM equations\n","        # .to('cpu')  # returns a copy of the tensor in CPU memory\n","        # .to('cuda:0')  # returns copy in CUDA memory, 0 indicates first GPU device\n","        # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to\n","\n","        # sequence length\n","        # size of input (10 or 1)\n","        sequence_length = in_put.size()[0]\n","        # print(\"in_put:\", in_put.shape)\n","\n","        # Run the LSTM\n","        # output - sequence of all the hidden states\n","        # hidden - most recent hidden state\n","        # input dimensions: [sequence_length, batch_size, 128]\n","        output, self.hidden = self.lstm(in_put.float().to(device), self.hidden)\n","        # output dimensions: [sequence_length, batch_size, 128]\n","        # outputs as many beats (sequence_length) as there were beats in the input\n","        # hidden: last hidden states from last beat\n","\n","        #########################\n","        # attention starts here #\n","        #########################\n","        \n","        # output without attention\n","        avg_output = output.view(sequence_length, batch_size, 128)  # reshape\n","        \n","        # if we're using a starter sequence, cut output to last note\n","        avg_output = avg_output[-1,:,:].unsqueeze(1)  # [batch_size, 1, 128]\n","        \n","        # return early (w/o attention) for base lstm\n","        if self.base_lstm:\n","          return avg_output.transpose(0,1), self.hidden\n","        \n","        #this variable holds the output after the attention has been applied.\n","        seqs = []\n","\n","        # slice the batched ssms to the right places\n","        beat_num = prev_sequence.shape[0]\n","        \n","        # find the row for this beat in each ssm\n","        # batched_ssm shape is (batch_size*beats, beats), bc all the pieces are stacked vertically atop each other\n","        inds_across_pieces = range(beat_num, batched_ssm.shape[0], batched_ssm.shape[1])  # eg 11, 2625, 105 - indices of this beat in each of the pieces in the batched_ssm\n","        # for the row for this beat in each ssm, slice the row up to (not including) this beat\n","        ssm_slice = batched_ssm[inds_across_pieces, :beat_num] # [batch_size, beat_num]\n","        # sparsemax makes entries in the vector add to 1\n","        weights = self.softmax(ssm_slice)  # weights are shape [batch_size, beat_num]\n","\n","        # this is the sparsemaxed SSM multiplied by the entire previous sequence\n","        # to scale the previous timesteps for how much attention to pay to each\n","        # TODO: replace .T\n","        weighted = (prev_sequence.permute(2,1,0)*weights).T  # [beat_num, batch_size, 128]\n","\n","        # then it's summed to provide weights for each note.\n","        weight_vec = (torch.sum(weighted, axis=0)).unsqueeze(1).to(device)  # [batch_size, 1, 128]\n","\n","        # This concatenates the weights for each note with the output for that note, which is then run through the linear layer to get the final output.\n","        # returns attentioned note\n","        pt2 = torch.hstack((weight_vec, avg_output)).transpose(1,2)\n","        attentioned = self.attention(pt2.float()).permute(2,0,1)  # before .permute() .to(\"cuda:0\")).to('cpu')\n","\n","        # delete vars to remove clutter in memory\n","        del pt2\n","        del weight_vec\n","        del weighted\n","        del weights\n","        del ssm_slice\n","        del inds_across_pieces\n","        del beat_num\n","        del avg_output\n","\n","        # return attentioned note\n","        return attentioned.double(), self.hidden  # hidden = hidden_state, cell_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quuG9ccfzegn"},"outputs":[],"source":["class model_trainer():\n","  def __init__(self, generator, optimizer, data, hidden_size=128, batch_size=50):\n","    self.generator = generator.to(\"cuda:0\")\n","    self.optimizer = optimizer\n","    self.batch_size = batch_size  # play with this\n","    self.hidden_size = hidden_size  # 128\n","    self.data = data\n","    self.data_length = data[0][0].shape[0]  # as long as piece length doesn't vary\n","    \n","  def train_epochs(self, num_epochs=50, full_training=False, variable_size_batches=False, save_name=\"model\"):\n","    #trains each epoch\n","    losslist = []\n","    #useful when you want to see the progression of the SSM over time\n","    piclist = []\n","\n","    for iter in tqdm(range(0, num_epochs)):\n","      # start training the generator\n","      self.generator.train()\n","      # use all data, and group batches by piece size\n","      batches = make_variable_size_batches(self.data, 1)\n","      \n","\n","      cum_loss = 0\n","      for batch_num in tqdm(range(len(batches))):\n","        batch = batches[batch_num]\n","        if full_training:\n","          # train on full-length pieces\n","          loss = self.train(batch)\n","        else:\n","          # train on first 105 beats of each piece\n","          loss = self.train(batch[:,:105,:])  # [batch, beats, 128]\n","        cum_loss+=loss\n","        del batch\n","        del loss\n","      del batches\n","          \n","      # print loss for early stopping\n","      print(cum_loss)\n","    \n","      # save generator after each epoch\n","      curr_file = f\"/exp/shager/music-gen/models/{save_name}-epoch-{str(iter)}-loss-{cum_loss:.5f}.txt\"\n","      # !touch curr_file\n","      torch.save(self.generator, curr_file)\n","\n","      # generate example piece for piclist\n","      #snap = self.generate_n_examples(n=1, length=95, starter_notes=10)\n","\n","      losslist.append(cum_loss) \n","      #piclist.append(snap)\n","        \n","      # early stopping:\n","      # after each epoch,\n","      # run w/ validation\n","      # if devset (validation) loss goes up for ~5 epochs in a row, early stopping\n","    return losslist, piclist\n","\n","  # train for one batch\n","  def train(self, batch, starter_notes=10):\n","    # seed vectors for the beginning:\n","    batch_size = batch.shape[0]\n","    self_sim = batch_SSM(batch.transpose(0,1), batch_size)  # use variable batch size\n","    sequence = batch[:,0:starter_notes,:].transpose(0,1)  # start w/ some amount of the piece - 10 might be a bit much\n","    generated = batch[:,0:starter_notes,:].transpose(0,1)\n","\n","    # reset hidden to zeros for each batch\n","    self.generator.init_hidden(batch_size)\n","        \n","    # zero the gradients before training for each batch\n","    self.optimizer.zero_grad()\n","    \n","    # for accumulating loss\n","    loss = 0\n","\n","    # first .forward on sequence of num_starter_beats (~5 or 10 or so)\n","    # then loop from there to generate one more element\n","    next_element = sequence.to(\"cpu\")  # make copy!\n","\n","    # take\n","    for i in range(0,batch.shape[1]-starter_notes):  # for each beat\n","      # iterate through beats, generating for each piece in the batch as you go\n","      val = torch.rand(1)  # probability it uses original - teacher forcing\n","\n","      # generate a beat for each piece in the batch\n","      # we need to do this even in cases of teacher forcing, so we can calculate loss\n","      output, _ = self.generator.forward(next_element, batch_size, sequence, self_sim)  # returns output, hidden - we don't need the latest copy of hidden\n","      # print(\"outside output:\", output.shape)\n","        \n","      if (val > .8):\n","        # teacher forcing - 20% of the time,  use original from piece instead of output\n","        next_element = batch[:,i+1,:].unsqueeze(0)  # [1, 0/deleted, 128] to [1, 1, 128]\n","      else:\n","        # 80% of the time we keep the output\n","        # take last output for each batch\n","        next_element = topk_batch_sample(output, 20) # sample up to 5 most likely notes at this beat\n","      \n","      # add next_element (either generated or teacher) to sequence\n","      sequence = torch.vstack((sequence, next_element.to(\"cpu\"))) # .unsqueeze(0)\n","      # append output (generated - not teacher forced) for loss\n","      generated = torch.vstack((generated, output.to('cpu')))  # used for loss\n","    \n","    # run loss after training on whole length of the pieces in the batches\n","    single_loss = custom_loss(generated[starter_notes:,:,:], batch.transpose(0,1)[starter_notes:,:,:])\n","    single_loss.backward()\n","\n","    # update the parameters of the LSTM after running on full batch\n","    self.optimizer.step()\n","\n","    loss += single_loss.detach().to('cpu')\n","    del next_element\n","    del self_sim\n","    del sequence\n","    del generated\n","    del single_loss\n","    return (loss)\n","\n","  def generate_n_pieces(self, initial_vectors, n_pieces, length, batched_ssm):\n","    # generates a batch of n new pieces of music\n","\n","    # freeze generator so it doesn't train anymore\n","    self.generator.eval().to(device)\n","    # start generator on random hidden states and cell states\n","    self.generator.set_random_hidden(n_pieces)\n","  \n","    # initial vectors in format [batch_size, num_notes=10, 128]\n","    # change sequence to [10, batch_size, 128]\n","    sequence = initial_vectors.transpose(0,1).to(device)\n","    next_element = sequence.to(device)\n","\n","    # can't generate more notes than the ssm has entries\n","    max_notes = batched_ssm.shape[0]-sequence.shape[0]\n","    \n","    # generate [length] more beats for the piece\n","    # or as many beats as available in the ssm\n","    for i in range(min(length, max_notes)):  # one at a time\n","      with torch.no_grad():\n","        # use n_pieces to generate as the batch size\n","        output, _ = self.generator.forward(next_element.float().to(device), n_pieces, sequence.to(device), batched_ssm.to(device))\n","        next_element = topk_batch_sample(output, 50)  # sample up to 5 most likely notes at this beat\n","      # add element to sequence\n","      sequence = torch.vstack((sequence, next_element.to(device)))\n","\n","    # return sequence of beats\n","    return sequence.to(\"cpu\")\n","\n","  def generate_n_examples(self, n=1, length=390, starter_notes=10, piece_inds=[0], random_source_pieces=False, batched_ssms=None):\n","    # get pieces from the data\n","    pieces = torch.vstack([self.data[i][0].unsqueeze(0) for i in piece_inds]) # get just the note for each piece, and stack pieces\n","    \n","    # print(pieces.shape)\n","\n","    # take first 10 notes in format [1, 10, 128]\n","    first_vecs = pieces[:,0:starter_notes,:]\n","\n","    # create batched SSMs for each piece\n","    if batched_ssms==None:\n","      batched_ssms = batch_SSM(pieces.transpose(0,1), n)\n","\n","    # generate pieces\n","    new_gen = self.generate_n_pieces(first_vecs, n, length, batched_ssms)\n","\n","    # clean up variables\n","    del pieces\n","    del first_vecs\n","    del batched_ssms\n","\n","    # return pieces\n","    return new_gen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wF37xHKXZKh6"},"outputs":[],"source":["best_model_base = torch.load(my_drive_path+\"trained/3-16-best-models/best-basic-model.txt\", map_location=torch.device('cpu'))\n","optimizer_lstm = None\n","hidden_size = 128\n","device = \"cpu\"\n","best_model_att = torch.load(my_drive_path+\"trained/3-16-best-models/best-attention-model.txt\", map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RffnBbwne-6w"},"outputs":[],"source":["def random_noise(original, length, starter_notes):\n","  g = torch.ones(1, length, 128)\n","  g = g/128\n","  return torch.vstack([original[0][:starter_notes],topk_batch_sample(g,50)[0].to(\"cuda:0\")])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v--VDxvCZ9Dq"},"outputs":[],"source":["def norm(SSM_unnorm):\n","  m = SSM_unnorm.mean()\n","  s = SSM_unnorm.std()\n","  return (SSM_unnorm-m)/s\n","def MSE(SSM1, SSM2):\n","  SSM1 = norm(torch.flatten(SSM1))\n","  SSM2 = norm(torch.flatten(SSM2))\n","  l = torch.nn.MSELoss()\n","  return l(SSM1, SSM2)\n","def test_similarity(data, att_model, base_model, num_to_test):\n","  similarity_attention = []\n","  similarity_base = []\n","  similarity_random = []\n","  for piece_info in tqdm(data):\n","    piece=piece_info[0].to(\"cuda:0\")\n","    original_SSM = SSM(piece)\n","    length = piece.shape[0]\n","    piece = piece.unsqueeze(0)\n","    trainer_att = model_trainer(att_model, None, [piece], 128)\n","    trainer_base = model_trainer(base_model, None, [piece], 128)\n","    for i in range(0,num_to_test):\n","      SSM_att = SSM(trainer_att.generate_n_examples(n=1, length=length-10, starter_notes=10, piece_inds=[0]).squeeze())\n","      SSM_base = SSM(trainer_base.generate_n_examples(n=1, length=length-10, starter_notes=10, piece_inds=[0]).squeeze())\n","      SSM_rand = SSM(random_noise(piece, length-10, 10))\n","      similarity_attention.append(MSE(original_SSM, SSM_att))\n","      similarity_base.append(MSE(original_SSM, SSM_base))\n","      similarity_random.append(MSE(original_SSM, SSM_rand))\n","  att = torch.tensor(similarity_attention)\n","  base = torch.tensor(similarity_base)\n","  rand = torch.tensor(similarity_random)\n","  print(\"Mean attention:\", att.mean())\n","  print(\"STD attention:\", att.std())\n","  print(\"Mean base:\", base.mean())\n","  print(\"STD base:\", base.std())\n","  print(\"Mean rand:\", rand.mean())\n","  print(\"STD rand:\", rand.std())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuHP9Yqq3VdZ"},"outputs":[],"source":["val_data= torch.load(my_drive_path + \"usable_data/mar-1-variable_bin_bounds_val.csv\")[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q9v-JPFwzkUu","outputId":"fe8bd640-bb8b-4424-f331-3b321f63f60f"},"outputs":[],"source":["test_similarity(val_data, best_model_att, best_model_base,3)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN6PbJqrtW7wcsyJdJOEhYl","mount_file_id":"1W7ac6eRqpQgxBgZQsdxAq2bHpL0nURSR","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
