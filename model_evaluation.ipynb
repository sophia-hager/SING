{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42d6e70",
   "metadata": {},
   "source": [
    "## Evaluate the Best Model\n",
    "The epoch of the model with the lowest loss,\n",
    "for both the attentioned and base LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nervous-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = False  # False for Local\n",
    "if drive:\n",
    "    !pip install pretty_midi\n",
    "    !pip install -U sparsemax\n",
    "# locally, also compile torch (with CUDA enabled if available):\n",
    "# conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gross-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi\n",
    "#this package is used to write it back into music.\n",
    "from mido import Message, MidiFile, MidiTrack\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.distributions\n",
    "import sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enhanced-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    my_drive_path = '/content/drive/MyDrive/2022_Special_Studies_Hablutzel/ModelCopies/'\n",
    "else: # local\n",
    "    my_drive_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model\n",
    "class music_generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, base_lstm=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size  # 128\n",
    "        # output_size is num expected features (128)\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers=1, bidirectional=False)\n",
    "        self.attention = nn.Linear(2, 1)\n",
    "        self.softmax = sparsemax.Sparsemax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden = None\n",
    "        self.base_lstm = base_lstm  # true to use lstm without attention\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # set hidden state to zeros after each batch\n",
    "        hidden = (torch.zeros(1, batch_size, self.hidden_size)).float() # .to(\"cuda:0\")  # [layers, batch_size, hidden_size/features]\n",
    "        self.hidden = (hidden, hidden) # hidden_state, cell_state\n",
    "        return\n",
    "\n",
    "    def set_random_hidden(self, batch_size):\n",
    "        # create new random hidden layer\n",
    "        hidden = (torch.randn(1, batch_size, self.hidden_size)).float() # .to(\"cuda:0\")\n",
    "        self.hidden = (hidden, hidden)\n",
    "        return\n",
    "\n",
    "    def forward(self, in_put, batch_size, prev_sequence, batched_ssm):\n",
    "        # look at tensor things - view vs. reshape vs. permute, and unsqueeze and squeeze\n",
    "        # try looking at the LSTM equations\n",
    "        # .to('cpu')  # returns a copy of the tensor in CPU memory\n",
    "        # .to('cuda:0')  # returns copy in CUDA memory, 0 indicates first GPU device\n",
    "        # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to\n",
    "\n",
    "        # sequence length\n",
    "        # size of input (10 or 1)\n",
    "        sequence_length = in_put.size()[0]\n",
    "        # print(\"in_put:\", in_put.shape)\n",
    "\n",
    "        # Run the LSTM\n",
    "        # output - sequence of all the hidden states\n",
    "        # hidden - most recent hidden state\n",
    "        # input dimensions: [sequence_length, batch_size, 128]\n",
    "        output, self.hidden = self.lstm(in_put.float(), self.hidden) # removed input.tocuda, so hidden doesn't need cuda either # make sure hidden in cuda memory! - #0].to(\"cuda:0\").float(), hidden[1].to(\"cuda:0\").float()))\n",
    "        # output dimensions: [sequence_length, batch_size, 128]\n",
    "        # outputs as many beats (sequence_length) as there were beats in the input\n",
    "        # hidden: last hidden states from last beat\n",
    "\n",
    "        #########################\n",
    "        # attention starts here #\n",
    "        #########################\n",
    "        \n",
    "        # output without attention\n",
    "        new_output = output.view(sequence_length, batch_size, 1, 128)  # reshape\n",
    "        avg_output = torch.sum(new_output, 2)\n",
    "        \n",
    "        # if we're using a starter sequence, cut output to last note\n",
    "        avg_output = avg_output[-1,:,:].unsqueeze(1)  # [batch_size, 1, 128]\n",
    "        \n",
    "        # return early (w/o attention) for base lstm\n",
    "        if self.base_lstm:\n",
    "          return avg_output.transpose(0,1), self.hidden\n",
    "        \n",
    "        #this variable holds the output after the attention has been applied.\n",
    "        seqs = []\n",
    "\n",
    "        # slice the batched ssms to the right places\n",
    "        beat_num = prev_sequence.shape[0]\n",
    "        \n",
    "        # find the row for this beat in each ssm\n",
    "        # batched_ssm shape is (batch_size*beats, beats), bc all the pieces are stacked vertically atop each other\n",
    "        inds_across_pieces = range(beat_num, batched_ssm.shape[0], batched_ssm.shape[1])  # eg 11, 2625, 105 - indices of this beat in each of the pieces in the batched_ssm\n",
    "        \n",
    "        # for the row for this beat in each ssm, slice the row up to (not including) this beat\n",
    "        ssm_slice = batched_ssm[inds_across_pieces, :beat_num] # [batch_size, beat_num]\n",
    "        \n",
    "        # sparsemax makes entries in the vector add to 1\n",
    "        weights = self.softmax(ssm_slice)  # weights are shape [batch_size, beat_num]\n",
    "\n",
    "        # this is the sparsemaxed SSM multiplied by the entire previous sequence\n",
    "        # to scale the previous timesteps for how much attention to pay to each\n",
    "        # TODO: replace .T\n",
    "        weighted = (prev_sequence.T*weights).T  # [batch_size, beat_num]\n",
    "\n",
    "        # then it's summed to provide weights for each note.\n",
    "        weight_vec = (torch.sum(weighted, axis=0)).unsqueeze(1)  # [batch_size, 1, 128]\n",
    "\n",
    "        # This concatenates the weights for each note with the output for that note, which is then run through the linear layer to get the final output.\n",
    "        # returns attentioned note\n",
    "        pt2 = torch.hstack((weight_vec, avg_output)).transpose(1,2)\n",
    "        attentioned = self.attention(pt2.float()).permute(2,0,1)  # before .permute() .to(\"cuda:0\")).to('cpu')\n",
    "\n",
    "        # delete vars to remove clutter in memory\n",
    "        del pt2\n",
    "        del weight_vec\n",
    "        del weighted\n",
    "        del weights\n",
    "        del ssm_slice\n",
    "        del inds_across_pieces\n",
    "        del beat_num\n",
    "        del new_output\n",
    "        del avg_output\n",
    "\n",
    "        # return attentioned note\n",
    "        return attentioned.double(), self.hidden  # hidden = hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fresh-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_trainer():\n",
    "  def __init__(self, generator, optimizer, data, hidden_size=128, batch_size=50):\n",
    "    self.generator = generator\n",
    "    self.optimizer = optimizer\n",
    "    self.batch_size = batch_size  # play with this\n",
    "    self.hidden_size = hidden_size  # 128\n",
    "    self.data = data\n",
    "    self.data_length = data[0][0].shape[0]  # as long as piece length doesn't vary\n",
    "\n",
    "  def train_epochs(self, num_epochs=50, full_training=False, variable_size_batches=False, save_name=\"model\"):\n",
    "    #trains each epoch\n",
    "    losslist = []\n",
    "    #useful when you want to see the progression of the SSM over time\n",
    "    piclist = []\n",
    "\n",
    "    for iter in tqdm(range(0, num_epochs)):\n",
    "      # start training the generator\n",
    "      self.generator.train()\n",
    "\n",
    "      if variable_size_batches:\n",
    "        # use all data, and group batches by piece size\n",
    "        batches = make_variable_size_batches(self.data)\n",
    "      elif full_training and not variable_size_batches: # truncating data doesn't work w/ variable size batches currently\n",
    "        # use all data\n",
    "        batches = make_batches(self.data, self.batch_size, self.data_length)\n",
    "      else:\n",
    "        # use first 100 pieces\n",
    "        # can we overfit on a small dataset? if so, can be a good thing b/c shows the model can learn\n",
    "        batches = make_batches(self.data[:100], self.batch_size, self.data_length)\n",
    "\n",
    "      cum_loss = 0\n",
    "      for batch_num in tqdm(range(len(batches))):\n",
    "        batch = batches[batch_num]\n",
    "        if full_training:\n",
    "          # train on full-length pieces\n",
    "          loss = self.train(batch)\n",
    "        else:\n",
    "          # train on first 105 beats of each piece\n",
    "          loss = self.train(batch[:,:105,:])  # [batch, beats, 128]\n",
    "        cum_loss+=loss\n",
    "        del batch\n",
    "        del loss\n",
    "      del batches\n",
    "          \n",
    "      # print loss for early stopping\n",
    "      print(cum_loss)\n",
    "    \n",
    "      # save generator after each epoch\n",
    "      curr_file = f\"{my_drive_path}trained/{save_name}-epoch-{str(iter)}-loss-{cum_loss:.5f}.txt\"\n",
    "      # !touch curr_file\n",
    "      torch.save(self.generator, curr_file)\n",
    "\n",
    "      # generate example piece for piclist\n",
    "      snap = self.generate_n_examples(n=1, length=95, starter_notes=10)\n",
    "\n",
    "      losslist.append(cum_loss) \n",
    "      piclist.append(snap)\n",
    "      del snap\n",
    "        \n",
    "      # early stopping:\n",
    "      # after each epoch,\n",
    "      # run w/ validation\n",
    "      # if devset (validation) loss goes up for ~5 epochs in a row, early stopping\n",
    "    return losslist, piclist\n",
    "\n",
    "  # train for one batch\n",
    "  def train(self, batch, starter_notes=10):\n",
    "    # seed vectors for the beginning:\n",
    "    batch_size = batch.shape[0]\n",
    "    self_sim = batch_SSM(batch.transpose(0,1), batch_size)  # use variable batch size\n",
    "    sequence = batch[:,0:starter_notes,:].transpose(0,1)  # start w/ some amount of the piece - 10 might be a bit much\n",
    "    generated = batch[:,0:starter_notes,:].transpose(0,1)\n",
    "\n",
    "    # reset hidden to zeros for each batch\n",
    "    self.generator.init_hidden(batch_size)\n",
    "        \n",
    "    # zero the gradients before training for each batch\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    # for accumulating loss\n",
    "    loss = 0\n",
    "\n",
    "    # first .forward on sequence of num_starter_beats (~5 or 10 or so)\n",
    "    # then loop from there to generate one more element\n",
    "    next_element = sequence.to(\"cpu\")  # make copy!\n",
    "\n",
    "    # take\n",
    "    for i in range(0,batch.shape[1]-starter_notes):  # for each beat\n",
    "      # iterate through beats, generating for each piece in the batch as you go\n",
    "      val = torch.rand(1)  # probability it uses original - teacher forcing\n",
    "\n",
    "      # generate a beat for each piece in the batch\n",
    "      # we need to do this even in cases of teacher forcing, so we can calculate loss\n",
    "      output, _ = self.generator.forward(next_element, batch_size, sequence, self_sim)  # returns output, hidden - we don't need the latest copy of hidden\n",
    "      # print(\"outside output:\", output.shape)\n",
    "        \n",
    "      if (val > .8):\n",
    "        # teacher forcing - 20% of the time,  use original from piece instead of output\n",
    "        next_element = batch[:,i+1,:].unsqueeze(0)  # [1, 0/deleted, 128] to [1, 1, 128]\n",
    "      else:\n",
    "        # 80% of the time we keep the output\n",
    "        # take last output for each batch\n",
    "        next_element = topk_batch_sample(output, 5) # sample up to 5 most likely notes at this beat\n",
    "      \n",
    "      # add next_element (either generated or teacher) to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\"))) # .unsqueeze(0)\n",
    "      # append output (generated - not teacher forced) for loss\n",
    "      generated = torch.vstack((generated, output))  # used for loss\n",
    "    \n",
    "    # run loss after training on whole length of the pieces in the batches\n",
    "    single_loss = custom_loss(generated[starter_notes:,:,:], batch.transpose(0,1)[starter_notes:,:,:])\n",
    "    single_loss.backward()\n",
    "\n",
    "    # update the parameters of the LSTM after running on full batch\n",
    "    self.optimizer.step()\n",
    "\n",
    "    loss += single_loss.detach().to('cpu')\n",
    "    del next_element\n",
    "    del self_sim\n",
    "    del sequence\n",
    "    del generated\n",
    "    del single_loss\n",
    "    return (loss)\n",
    "\n",
    "#   def generate_n_pieces_old(self, initial_vectors, n_pieces, length, batched_ssm):\n",
    "#     # generates a batch of n new pieces of music\n",
    "\n",
    "#     # freeze generator so it doesn't train anymore\n",
    "#     self.generator.eval()  \n",
    "#     # start generator on random hidden states and cell states\n",
    "#     self.generator.set_random_hidden(n_pieces)\n",
    "  \n",
    "#     # initial vectors in format [batch_size, num_notes=10, 128]\n",
    "#     # change sequence to [10, batch_size, 128]\n",
    "#     sequence = initial_vectors.transpose(0,1)\n",
    "    \n",
    "#     # can't generate more notes than the ssm\n",
    "#     max_notes = batched_ssm.shape[0]\n",
    "    \n",
    "#     # generate [length] more beats for the piece\n",
    "#     for i in range(0, min(length, max_notes)):  # one at a time\n",
    "#       print(i)\n",
    "#       with torch.no_grad():\n",
    "#         # use n_pieces to generate as the batch size\n",
    "#         output, _ = self.generator.forward(sequence.float(), n_pieces, sequence, batched_ssm)\n",
    "#         next_element = topk_batch_sample(output, 5)  # sample up to 5 most likely notes at this beat\n",
    "#       # add element to sequence\n",
    "#       sequence = torch.vstack((sequence, next_element.to(\"cpu\")))\n",
    "\n",
    "#     # return sequence of beats\n",
    "#     return sequence\n",
    "\n",
    "  def generate_n_pieces(self, initial_vectors, n_pieces, length, batched_ssm):\n",
    "    # generates a batch of n new pieces of music\n",
    "\n",
    "    # freeze generator so it doesn't train anymore\n",
    "    self.generator.eval()  \n",
    "    # start generator on random hidden states and cell states\n",
    "    self.generator.set_random_hidden(n_pieces)\n",
    "  \n",
    "    # initial vectors in format [batch_size, num_notes=10, 128]\n",
    "    # change sequence to [10, batch_size, 128]\n",
    "    sequence = initial_vectors.transpose(0,1)\n",
    "    next_element = sequence.to(\"cpu\")\n",
    "\n",
    "    # can't generate more notes than the ssm has entries\n",
    "    max_notes = batched_ssm.shape[0]-sequence.shape[0]\n",
    "    \n",
    "    # generate [length] more beats for the piece\n",
    "    # or as many beats as available in the ssm\n",
    "    for i in range(min(length, max_notes)):  # one at a time\n",
    "      with torch.no_grad():\n",
    "        # use n_pieces to generate as the batch size\n",
    "        output, _ = self.generator.forward(next_element.float(), n_pieces, sequence, batched_ssm)\n",
    "        next_element = topk_batch_sample(output, 5)  # sample up to 5 most likely notes at this beat\n",
    "      # add element to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\")))\n",
    "\n",
    "    # return sequence of beats\n",
    "    return sequence\n",
    "  \n",
    "#   def generate_n_examples_old(self, n=1, length=390, starter_notes=10, source_piece=0):\n",
    "#     # get piece from the data\n",
    "#     piece = self.data[source_piece][0].unsqueeze(0)  # in format [1, 400, 128]\n",
    "\n",
    "#     # take first 10 notes in format [1, 10, 128]\n",
    "#     first_vec = piece[:,0:starter_notes,:]\n",
    "\n",
    "#     # create batched SSMs for each piece\n",
    "#     batched_ssms = batch_SSM(piece.transpose(0,1), n)\n",
    "\n",
    "#     # generate pieces\n",
    "#     new_gen = self.generate_n_pieces(first_vec, n, length, batched_ssms)\n",
    "\n",
    "#     # clean up variables\n",
    "#     del piece\n",
    "#     del first_vec\n",
    "#     del batched_ssms\n",
    "\n",
    "#     # return pieces\n",
    "#     return new_gen\n",
    "\n",
    "  def generate_n_examples(self, n=1, length=390, starter_notes=10, piece_inds=[0], random_source_pieces=False):\n",
    "    # get pieces from the data\n",
    "    pieces = torch.vstack([self.data[i][0].unsqueeze(0) for i in piece_inds]) # get just the note for each piece, and stack pieces\n",
    "    \n",
    "    # print(pieces.shape)\n",
    "\n",
    "    # take first 10 notes in format [1, 10, 128]\n",
    "    first_vecs = pieces[:,0:starter_notes,:]\n",
    "\n",
    "    # create batched SSMs for each piece\n",
    "    batched_ssms = batch_SSM(pieces.transpose(0,1), n)\n",
    "\n",
    "    # generate pieces\n",
    "    new_gen = self.generate_n_pieces(first_vecs, n, length, batched_ssms)\n",
    "\n",
    "    # clean up variables\n",
    "    del pieces\n",
    "    del first_vecs\n",
    "    del batched_ssms\n",
    "\n",
    "    # return pieces\n",
    "    return new_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "motivated-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in the piece of music and returns the chroma vectors\n",
    "def get_chroma(roll, length):\n",
    "    chroma_matrix = torch.zeros((roll.size()[0],12))\n",
    "    for note in range(0, 12):\n",
    "        chroma_matrix[:, note] = torch.sum(roll[:, note::12], axis=1)\n",
    "    return chroma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inner-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes in the sequence and creates a self-similarity matrix (it calls chroma function inside)\n",
    "def SSM(sequence):\n",
    "  #tensor will be in form length, hidden_size (128)\n",
    "  cos = nn.CosineSimilarity(dim=1)\n",
    "  chrom = get_chroma(sequence, sequence.size()[0])\n",
    "  len = chrom.size()[0]\n",
    "  SSM=torch.zeros((len, len))\n",
    "  for i in range(0, len):\n",
    "    SSM[i] = cos(chrom[i].view(1, -1),chrom)\n",
    "  return (SSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informational-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this bundles the SSM function.\n",
    "def batch_SSM(seq, batch_size):\n",
    "  # takes sequence in format\n",
    "  # [beats=400, batch_size, 128]\n",
    "  # print(\"SSM\\tsequence_shape\", seq.shape)\n",
    "  SSMs = []\n",
    "  for i in range(0, batch_size):\n",
    "    # print(\"SSM\\tsequence\", seq[:,i,:].shape)\n",
    "    ssm = SSM(seq[:,i,:])  # [beats, batch, 128]\n",
    "    # print(\"SSM\\tssm\", ssm.shape)\n",
    "    SSMs.append(ssm)  \n",
    "  return torch.vstack(SSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stylish-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling function \n",
    "def topk_sample_one(sequence, k):\n",
    "  #takes in size sequence length, batch size, values\n",
    "  softmax = sparsemax.Sparsemax(dim=2)\n",
    "  vals, indices = torch.topk(sequence[:, :, 20:108],k)\n",
    "  indices+=20\n",
    "  seq = torch.distributions.Categorical(softmax(vals.float()))\n",
    "  samples = seq.sample()\n",
    "  onehot = F.one_hot(torch.gather(indices, -1, samples.unsqueeze(-1)), num_classes = sequence.shape[2]).squeeze(dim=2)\n",
    "  return(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "valued-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples multiple times for the time-step\n",
    "def topk_batch_sample(sequence, k):\n",
    "  for i in range(0, 3):\n",
    "    new= topk_sample_one(sequence, k)\n",
    "    if i ==0:\n",
    "      sum = new\n",
    "    else:\n",
    "      sum+=new\n",
    "  return(torch.where(sum>0, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target):\n",
    "  # custom loss function\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  weighted_mse = criterion(output.double(), target.double())\n",
    "  batch_size = output.size()[1]\n",
    "  ssm_err = 0\n",
    "  for i in range(0, batch_size):\n",
    "    SSM1 = SSM(output[:,i,:])\n",
    "    SSM2 = SSM(target[:,i,:])\n",
    "    ssm_err += (torch.sum((SSM1-SSM2)**2)/(SSM2.size(0)**2))\n",
    "\n",
    "\n",
    "  return torch.sum(weighted_mse)+ssm_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stupid-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns batches where piece size is constant within the batch\n",
    "# but piece size is different across batches\n",
    "# and batches are in random order\n",
    "def make_variable_size_batches(data, min_batch_size=10):\n",
    "  # sort data by num beats (element at index 2 in each sublist)\n",
    "  data.sort(key = lambda x: x[2], reverse=False)  # sort descending\n",
    "\n",
    "  # split data into batches, where each batch contains pieces of the same size\n",
    "  batches = []\n",
    "\n",
    "  i = 0  # counter of pieces\n",
    "  \n",
    "  while i < len(data):\n",
    "    this_batch = []\n",
    "    pieces_this_batch = 0\n",
    "    current_beats = data[i][2] # num beats in this batch\n",
    "\n",
    "    # for all pieces with this # of beats\n",
    "    while i < len(data) and data[i][2] == current_beats:\n",
    "      # get tensor from row of data, and reshape \n",
    "      just_tensor = data[i][0].view(1, data[i][0].shape[0], 128)  \n",
    "      this_batch.append(just_tensor)\n",
    "\n",
    "      # increment counters\n",
    "      i += 1\n",
    "      pieces_this_batch += 1\n",
    "\n",
    "    # print(\"this batch\", this_batch)\n",
    "    # print(\"shapes in batch\")\n",
    "    # for p in this_batch:\n",
    "      # print(\"\\t\", p.shape)\n",
    "        \n",
    "    # only save large enough batches\n",
    "    if pieces_this_batch >= min_batch_size:\n",
    "        # reformat pieces in this batch into one tensor of size [batch size, beats, 128]\n",
    "        batch = torch.cat(this_batch, dim=0)\n",
    "\n",
    "        # store batch\n",
    "        batches.append(batch)\n",
    "\n",
    "    # clean up variables\n",
    "    del this_batch\n",
    "    del pieces_this_batch\n",
    "    del current_beats\n",
    "\n",
    "  # randomize batches order\n",
    "  random.shuffle(batches)\n",
    "\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "violent-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in the batch size and data and returns batches of the batch size\n",
    "def make_batches(data, batch_size, piece_size=800, slice_data=False):\n",
    "  # slice data to piece_size\n",
    "  if slice_data:\n",
    "    data = [(roll[0][:piece_size], roll[1], roll[2]) for roll in data if len(roll[0]) >= piece_size]\n",
    "    # print(f\"sliced pieces to length {piece_size}; total pieces is now {len(data)}\")\n",
    "    \n",
    "  random.shuffle(data)\n",
    "  batches = []\n",
    "  if batch_size > 1:  # make batches\n",
    "    num_batches = len(data)//batch_size\n",
    "    for i in range(0, num_batches):\n",
    "      # take just the tensors, and vstack\n",
    "      batch = torch.cat([roll[0] for roll in data[i*batch_size: (i+1)*(batch_size)]]).view(batch_size, piece_size, 128)\n",
    "      batches.append(batch)\n",
    "  else:  # each piece is its own batch - doesn't use passed-in piece_size\n",
    "    for i in range(len(data)):\n",
    "      # removes tempo info from data, but leaves 1 piece per batch\n",
    "      piece_size = data[i][0].shape[0]\n",
    "      batch = data[i][0].view(1, piece_size, 128)\n",
    "      batches.append(batch)\n",
    "      # print(batches[i])\n",
    "  # print(batches)\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "worthy-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_in_directory(filepath):\n",
    "    # get the names of all files, sorted by epoch\n",
    "    files = os.listdir(my_drive_path + filepath)\n",
    "    files_by_epoch = [(name, int(name.split(\"-\")[2])) for name in files if 'model' in name]  # parse out epoch\n",
    "    files_by_epoch.sort(key = lambda x: x[1])  # sort by epoch\n",
    "    \n",
    "    # load model at each epoch\n",
    "    files_to_load = [my_drive_path + filepath + filename[0] for filename in files_by_epoch]  # paths to each file\n",
    "    # print(\"loading:\\n\", files_to_load, sep=\"\")\n",
    "    models = [torch.load(filename) for filename in files_to_load]  # load models\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac0b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_by_epoch(filepath=\"trained/attention_model_v3/\", epoch=15):\n",
    "    # get the model for a specific epoch\n",
    "    files = os.listdir(my_drive_path + filepath)  # list all files in the directory\n",
    "    my_file = [name for name in files if f\"epoch-{epoch}\" in name][0]  # find the filename for this epoch\n",
    "    \n",
    "    return torch.load(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "early-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_att = get_models_in_directory(\"trained/attention_model_v2/\")\n",
    "# models_lstm = get_models_in_directory(\"trained/lstm_v2/\")\n",
    "best_epoch_att = 17\n",
    "best_epoch_lstm = 28\n",
    "#best_model_att = get_model_by_epoch(filepath=\"trained/attention_model_v3/\", epoch=best_epoch_att)\n",
    "#best_model_lstm = get_model_by_epoch(filepath=\"trained/lstm_v3/\", epoch=best_epoch_lstm)\n",
    "best_model_att = torch.load(\"trained/attention_model_v3/model-epoch-17-loss-363.88825.txt\")\n",
    "best_model_lstm = torch.load(\"trained/lstm_v3/model_lstm-epoch-28-loss-93.15963.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latter-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "music_generator(\n",
       "  (lstm): LSTM(128, 128)\n",
       "  (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (softmax): Sparsemax(dim=1)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "herbal-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "music_generator(\n",
       "  (lstm): LSTM(128, 128)\n",
       "  (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (softmax): Sparsemax(dim=1)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe78cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_att = torch.optim.Adam(best_model_att.parameters(), lr=0.005)\n",
    "optimizer_lstm = torch.optim.Adam(best_model_lstm.parameters(), lr=0.005)\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amazing-contact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff105785c70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load validation data\n",
    "test_data = torch.load(my_drive_path + \"usable_data/test_tempo_all_w_beats_30.csv\") \n",
    "# val_data = torch.load(my_drive_path + \"usable_data/validation_tempo_round_down_30.csv\")\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e9ee890",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_att = model_trainer(best_model_att, optimizer_att, test_data, hidden_size)\n",
    "trainer_lstm = model_trainer(best_model_lstm, optimizer_lstm, test_data, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b583287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.model_trainer at 0x7ff10779e3d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "written-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_val_pieces(trainer, val_batch, starter_notes=10):\n",
    "    # how many pieces in batch\n",
    "    n = val_batch.shape[0]\n",
    "    \n",
    "    # length of the pieces in this batch\n",
    "    length = val_batch.shape[1]\n",
    "    \n",
    "    # take first 10 notes in format [1, 10, 128]\n",
    "    first_vecs = val_batch[:,0:starter_notes,:]\n",
    "\n",
    "    # create batched SSMs for each piece\n",
    "    batched_ssms = batch_SSM(val_batch.transpose(0,1), n)\n",
    "\n",
    "    # generate pieces\n",
    "    # REMOVE STARTER NOTES ONCE UPDATE FUNCTION\n",
    "    generated = trainer.generate_n_pieces(first_vecs, n, length - starter_notes, batched_ssms)\n",
    "    \n",
    "    # get loss\n",
    "    single_loss = custom_loss(generated[starter_notes:,:,:], val_batch.transpose(0,1)[starter_notes:,:,:])\n",
    "    #single_loss.backward()\n",
    "    loss = single_loss.detach()\n",
    "\n",
    "    # clean up variables\n",
    "    del n\n",
    "    del length\n",
    "    del first_vecs\n",
    "    del batched_ssms\n",
    "    del single_loss\n",
    "\n",
    "    # return pieces and loss\n",
    "    return generated, float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "significant-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_on_data(trainer, data, val_size=450, variable_batches=False):\n",
    "    \"\"\"Get loss when generating on given data, generating with a given model\"\"\"\n",
    "    # make batches from val_data\n",
    "    # so we can generate for all pieces of the same length at once\n",
    "    if variable_batches:\n",
    "        batches = make_variable_size_batches(data, min_batch_size=0)\n",
    "    else:\n",
    "        # batch size 50 for speeeed (we hopeee)\n",
    "        # use slice_data = True so variable-size data gets grouped together\n",
    "        batches = make_batches(data, 50, piece_size=val_size, slice_data=True)\n",
    "    \n",
    "    # accumulate loss for each batch\n",
    "    cum_loss = 0\n",
    "    for batch_num in tqdm(range(len(batches))): # tqdm(range(len(batches))):\n",
    "        val_batch = batches[batch_num]\n",
    "        # create pieces to get loss\n",
    "        generated, loss = generate_val_pieces(trainer, val_batch)\n",
    "        # print(\"\\tloss:\", loss)\n",
    "        cum_loss += loss\n",
    "        del val_batch\n",
    "        del loss\n",
    "        del generated\n",
    "        \n",
    "    return cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sought-consistency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]<ipython-input-4-1877afbe48f8>:81: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1666647174771/work/aten/src/ATen/native/TensorShape.cpp:3281.)\n",
      "  weighted = (prev_sequence.T*weights).T  # [batch_size, beat_num]\n",
      "100%|██████████| 70/70 [08:31<00:00,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.76913072807886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_att = get_loss_on_data(trainer_att, test_data, variable_batches=True)  # True takes forever; tests full pieces\n",
    "print(loss_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fresh-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:35<00:00,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.0691560879751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_lstm = get_loss_on_data(trainer_lstm, test_data, variable_batches=True)  # True takes forever; tests full pieces\n",
    "print(loss_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349992e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
