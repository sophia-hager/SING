{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nervous-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = False  # False for Local\n",
    "if drive:\n",
    "    !pip install pretty_midi\n",
    "    !pip install -U sparsemax\n",
    "# locally, also compile torch with CUDA enabled:\n",
    "# conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gross-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi\n",
    "#this package is used to write it back into music.\n",
    "from mido import Message, MidiFile, MidiTrack\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.distributions\n",
    "import sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enhanced-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    my_drive_path = '/content/drive/MyDrive/2022_Special_Studies_Hablutzel/ModelCopies/'\n",
    "else: # local\n",
    "    my_drive_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the model\n",
    "class music_generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, base_lstm=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size  # 128\n",
    "        # output_size is num expected features (128)\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers=1, bidirectional=False)\n",
    "        self.attention = nn.Linear(2, 1)\n",
    "        self.softmax = sparsemax.Sparsemax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden = None\n",
    "        self.base_lstm = base_lstm  # true to use lstm without attention\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # set hidden state to zeros after each batch\n",
    "        hidden = (torch.zeros(1, batch_size, self.hidden_size)).float() # .to(\"cuda:0\")  # [layers, batch_size, hidden_size/features]\n",
    "        self.hidden = (hidden, hidden) # hidden_state, cell_state\n",
    "        return\n",
    "\n",
    "    def set_random_hidden(self, batch_size):\n",
    "        # create new random hidden layer\n",
    "        hidden = (torch.randn(1, batch_size, self.hidden_size)).float() # .to(\"cuda:0\")\n",
    "        self.hidden = (hidden, hidden)\n",
    "        return\n",
    "\n",
    "    def forward(self, input, batch_size, prev_sequence, batched_ssm):\n",
    "        # look at tensor things - view vs. reshape vs. permute, and unsqueeze and squeeze\n",
    "        # try looking at the LSTM equations\n",
    "        # .to('cpu')  # returns a copy of the tensor in CPU memory\n",
    "        # .to('cuda:0')  # returns copy in CUDA memory, 0 indicates first GPU device\n",
    "        # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to\n",
    "\n",
    "        # sequence length\n",
    "        # size of input (10 or 1)\n",
    "        sequence_length = input.size()[0]\n",
    "\n",
    "        # Run the LSTM\n",
    "        # output - sequence of all the hidden states\n",
    "        # hidden - most recent hidden state\n",
    "        # input dimensions: [sequence_length, batch_size, 128]\n",
    "        output, self.hidden = self.lstm(input.float(), self.hidden) # removed input.tocuda, so hidden doesn't need cuda either # make sure hidden in cuda memory! - #0].to(\"cuda:0\").float(), hidden[1].to(\"cuda:0\").float()))\n",
    "        # output dimensions: [sequence_length, batch_size, 128]\n",
    "        # outputs as many beats (sequence_length) as there were beats in the input\n",
    "        # hidden: last hidden states from last beat\n",
    "\n",
    "        # Get copy of the output\n",
    "        # take out this line after finish LSTM (other than reshaping) - unsqueeze to get right reshaping\n",
    "        output_1 = output.to('cpu') # [-5:,:,:]  # don't just take last 5\n",
    "\n",
    "        #########################\n",
    "        # attention starts here #\n",
    "        #########################\n",
    "        \n",
    "        # output without attention\n",
    "        new_output = output_1.view(sequence_length, batch_size, 1, 128)  # reshape\n",
    "        avg_output = torch.sum(new_output, 2)  # return this for base LSTM (w/o attention)\n",
    "\n",
    "        # return early (w/o attention) for base lstm\n",
    "        if self.base_lstm:\n",
    "          return avg_output, self.hidden  # TODO: test return type\n",
    "        \n",
    "        #this variable holds the output after the attention has been applied.\n",
    "        seqs = []\n",
    "\n",
    "        # slice the batched ssms to the right places\n",
    "        beat_num = prev_sequence.shape[0]\n",
    "        # find the row for this beat in each ssm\n",
    "        # batched_ssm shape is (batch_size*beats, beats), bc all the pieces are stacked vertically atop each other\n",
    "        inds_across_pieces = range(beat_num, batched_ssm.shape[0], batched_ssm.shape[1])  # eg 11, 2625, 105 - indices of this beat in each of the pieces in the batched_ssm\n",
    "        # for the row for this beat in each ssm, slice the row up to (not including) this beat\n",
    "        ssm_slice = batched_ssm[inds_across_pieces, :beat_num]\n",
    "        \n",
    "        # sparsemax makes entries in the vector add to 1\n",
    "        weights = self.softmax(ssm_slice)\n",
    "        # print(\"weights shape:\", weights.shape)\n",
    "\n",
    "        # this is the sparsemaxed SSM multiplied by the entire previous sequence\n",
    "        # to scale the previous timesteps for how much attention to pay to each\n",
    "        weighted = (prev_sequence.T*weights).T # replace .T - see which dimensions we're switching\n",
    "\n",
    "        # then it's summed to provide weights for each note.\n",
    "        weight_vec = (torch.sum(weighted, axis=0))\n",
    "        #This concatenates the weights for each note with the output for that note, which is then run through the linear layer to get the final output.\n",
    "        pt2 = torch.hstack((weight_vec.unsqueeze(1), avg_output[0,:,:].unsqueeze(1))).transpose(1,2)\n",
    "        attentioned = self.attention(pt2.float()).permute(2,0,1)  # before .permute() .to(\"cuda:0\")).to('cpu')\n",
    "\n",
    "        # final added new output to seqs.\n",
    "        seqs.append(attentioned)\n",
    "\n",
    "        # seqs is combined into one tensor and returned, along with the hidden and cell states.\n",
    "        newoutput = torch.vstack(seqs)\n",
    "\n",
    "        # delete vars to remove clutter in memory\n",
    "        del seqs\n",
    "        del pt2\n",
    "        del attentioned\n",
    "        del weight_vec\n",
    "        del weighted\n",
    "        del weights\n",
    "        del ssm_slice\n",
    "        del inds_across_pieces\n",
    "        del beat_num\n",
    "        del new_output\n",
    "        del avg_output\n",
    "\n",
    "        return newoutput.double(), self.hidden  # hidden = hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fresh-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_trainer():\n",
    "  def __init__(self, generator, optimizer, data, hidden_size=128, batch_size=50):\n",
    "    self.generator = generator\n",
    "    self.optimizer = optimizer\n",
    "    self.batch_size = batch_size  # play with this\n",
    "    self.hidden_size = hidden_size  # 128\n",
    "    self.data = data\n",
    "    self.data_length = data[0][0].shape[0]  # as long as piece length doesn't vary\n",
    "\n",
    "  def train_epochs(self, num_epochs=50, full_training=False, variable_size_batches=False, save_name=\"model\"):\n",
    "    #trains each epoch\n",
    "    losslist = []\n",
    "    #useful when you want to see the progression of the SSM over time\n",
    "    piclist = []\n",
    "\n",
    "    for iter in tqdm(range(0, num_epochs)):\n",
    "      # start training the generator\n",
    "      generator.train()\n",
    "\n",
    "      if full_training and variable_size_batches:\n",
    "        # use all data, and group batches by piece size\n",
    "        batches = make_variable_size_batches(self.data)\n",
    "      elif not full_training and variable_size_batches:\n",
    "        batches = make_variable_size_batches(self.data[:100])  # buggy\n",
    "      elif full_training:\n",
    "        # use all data\n",
    "        batches = make_batches(self.data, self.batch_size, self.data_length)\n",
    "      else:\n",
    "        # use first 100 pieces\n",
    "        # can we overfit on a small dataset? if so, can be a good thing b/c shows the model can learn\n",
    "        batches = make_batches(self.data[:100], self.batch_size, self.data_length)\n",
    "\n",
    "      cum_loss = 0\n",
    "      for batch_num in tqdm(range(len(batches))):\n",
    "        batch = batches[batch_num]\n",
    "        if full_training:\n",
    "          # train on full-length pieces\n",
    "          loss = self.train(batch)\n",
    "        else:\n",
    "          # train on first 105 beats of each piece\n",
    "          loss = self.train(batch[:,:105,:])  # [batch, beats, 128]\n",
    "        cum_loss+=loss\n",
    "        del batch\n",
    "        del loss\n",
    "      del batches\n",
    "          \n",
    "      # print loss for early stopping\n",
    "      print(cum_loss)\n",
    "    \n",
    "      # save generator after each epoch\n",
    "      curr_file = f\"{my_drive_path}trained/{save_name}-epoch-{str(iter)}-loss-{cum_loss:.5f}.txt\"\n",
    "      !touch curr_file\n",
    "      torch.save(generator, curr_file)\n",
    "\n",
    "      # generate example piece for piclist\n",
    "      snap = self.generate_n_examples(n=1, length=95, starter_notes=10)\n",
    "\n",
    "      losslist.append(cum_loss) \n",
    "      piclist.append(snap)\n",
    "      del snap\n",
    "        \n",
    "      # early stopping:\n",
    "      # after each epoch,\n",
    "      # run w/ validation\n",
    "      # if devset (validation) loss goes up for ~5 epochs in a row, early stopping\n",
    "    return losslist, piclist\n",
    "\n",
    "  # train for one batch\n",
    "  def train(self, batch, starter_notes=10):\n",
    "    #seed vectors for the beginning:\n",
    "    batch_size = batch.shape[0]\n",
    "    self_sim = batch_SSM(batch.transpose(0,1), batch_size)  # use variable batch size\n",
    "    sequence = batch[:,0:starter_notes,:].transpose(0,1)  # start w/ some amount of the piece - 10 might be a bit much\n",
    "    generated = batch[:,0:starter_notes,:].transpose(0,1)\n",
    "\n",
    "    # reset hidden to zeros for each batch\n",
    "    generator.init_hidden(batch_size)\n",
    "        \n",
    "    # zero the gradients before training for each batch\n",
    "    self.optimizer.zero_grad()\n",
    "    \n",
    "    # for accumulating loss\n",
    "    loss = 0\n",
    "\n",
    "    # first .forward on sequence of num_starter_beats (~5 or 10 or so)\n",
    "    # then loop from there to generate one more element\n",
    "    next_element = sequence\n",
    "\n",
    "    # take\n",
    "    for i in range(0,batch.shape[1]-starter_notes):  # for each beat\n",
    "      # iterate through beats num_starter_beats-400, generating for each piece in the batch as you go\n",
    "      val = torch.rand(1)  # probability it uses original - teacher forcing\n",
    "\n",
    "      # generate a beat for each piece in the batch\n",
    "      # we need to do this even in cases of teacher forcing, so we can calculate loss\n",
    "      output, _ = self.generator.forward(next_element, batch_size, sequence, self_sim)  # returns output, hidden - we don't need the latest copy of hidden\n",
    "      \n",
    "      if (val > .8):\n",
    "        # teacher forcing - 20% of the time,  use original from piece instead of output\n",
    "        next_element = batch[:,i+1,:].unsqueeze(0)  # [1, 0/deleted, 128] to [1, 1, 128]\n",
    "      else:\n",
    "        # 80% of the time we keep the output\n",
    "        # take last output for each batch\n",
    "        next_element = topk_batch_sample(output, 1)\n",
    "      \n",
    "      # add next_element (either generated or teacher) to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\"))) # .unsqueeze(0)\n",
    "      # append output (generated - not teacher forced) for loss\n",
    "      generated = torch.vstack((generated, output))  # used for loss\n",
    "    \n",
    "    # run loss after training on whole length of the pieces in the batches\n",
    "    single_loss = custom_loss(generated[starter_notes:,:,:], batch.transpose(0,1)[starter_notes:,:,:])\n",
    "    single_loss.backward()\n",
    "\n",
    "    # update the parameters of the LSTM after running on full batch\n",
    "    self.optimizer.step()\n",
    "\n",
    "    loss += single_loss.detach().to('cpu')\n",
    "    del next_element\n",
    "    del self_sim\n",
    "    del sequence\n",
    "    del generated\n",
    "    del single_loss\n",
    "    return (loss)\n",
    "\n",
    "  def generate_n_pieces(self, initial_vectors, n_pieces, length, batched_ssm):\n",
    "    # generates a batch of n new pieces of music\n",
    "\n",
    "    # freeze generator so it doesn't train anymore\n",
    "    self.generator.eval()  \n",
    "    # start generator on random hidden states and cell states\n",
    "    self.generator.set_random_hidden(n_pieces)\n",
    "  \n",
    "    # initial vectors in format [batch_size, num_notes=10, 128]\n",
    "    # change sequence to [10, batch_size, 128]\n",
    "    sequence = initial_vectors.transpose(0,1)\n",
    "    \n",
    "    # generate [length] more beats for the piece\n",
    "    for i in range(0, length):  # one at a time\n",
    "      with torch.no_grad():\n",
    "        # use n_pieces to generate as the batch size\n",
    "        output, _ = self.generator.forward(sequence.float(), n_pieces, sequence, batched_ssm)\n",
    "        next_element = topk_batch_sample(output, 1)\n",
    "      # add element to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\")))\n",
    "\n",
    "    # return sequence of beats\n",
    "    return sequence\n",
    "  \n",
    "  def generate_n_examples(self, n=1, length=390, starter_notes=10, source_piece=0):\n",
    "    # get piece from the data\n",
    "    piece = self.data[source_piece][0].unsqueeze(0)  # in format [1, 400, 128]\n",
    "\n",
    "    # take first 10 notes in format [1, 10, 128]\n",
    "    first_vec = piece[:,0:starter_notes,:]\n",
    "\n",
    "    # create batched SSMs for each piece\n",
    "    batched_ssms = batch_SSM(piece.transpose(0,1), n)\n",
    "\n",
    "    # generate pieces\n",
    "    new_gen = self.generate_n_pieces(first_vec, n, length, batched_ssms)\n",
    "\n",
    "    # clean up variables\n",
    "    del piece\n",
    "    del first_vec\n",
    "    del batched_ssms\n",
    "\n",
    "    # return pieces\n",
    "    return new_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "motivated-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in the piece of music and returns the chroma vectors\n",
    "def get_chroma(roll, length):\n",
    "    chroma_matrix = torch.zeros((roll.size()[0],12))\n",
    "    for note in range(0, 12):\n",
    "        chroma_matrix[:, note] = torch.sum(roll[:, note::12], axis=1)\n",
    "    return chroma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inner-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes in the sequence and creates a self-similarity matrix (it calls chroma function inside)\n",
    "def SSM(sequence):\n",
    "  #tensor will be in form length, hidden_size (128)\n",
    "  cos = nn.CosineSimilarity(dim=1)\n",
    "  chrom = get_chroma(sequence, sequence.size()[0])\n",
    "  len = chrom.size()[0]\n",
    "  SSM=torch.zeros((len, len))\n",
    "  for i in range(0, len):\n",
    "    SSM[i] = cos(chrom[i].view(1, -1),chrom)\n",
    "  return (SSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informational-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this bundles the SSM function.\n",
    "def batch_SSM(seq, batch_size):\n",
    "  # takes sequence in format\n",
    "  # [beats=400, batch_size, 128]\n",
    "  # print(\"SSM\\tsequence_shape\", seq.shape)\n",
    "  SSMs = []\n",
    "  for i in range(0, batch_size):\n",
    "    # print(\"SSM\\tsequence\", seq[:,i,:].shape)\n",
    "    ssm = SSM(seq[:,i,:])  # [beats, batch, 128]\n",
    "    # print(\"SSM\\tssm\", ssm.shape)\n",
    "    SSMs.append(ssm)  \n",
    "  return torch.vstack(SSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stylish-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling function \n",
    "def topk_sample_one(sequence, k):\n",
    "  #takes in size sequence length, batch size, values\n",
    "  softmax = sparsemax.Sparsemax(dim=2)\n",
    "  vals, indices = torch.topk(sequence[:, :, 20:108],k)\n",
    "  indices+=20\n",
    "  seq = torch.distributions.Categorical(softmax(vals.float()))\n",
    "  samples = seq.sample()\n",
    "  onehot = F.one_hot(torch.gather(indices, -1, samples.unsqueeze(-1)), num_classes = sequence.shape[2]).squeeze(dim=2)\n",
    "  return(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "valued-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples multiple times for the time-step\n",
    "def topk_batch_sample(sequence, k):\n",
    "  for i in range(0, 3):\n",
    "    new= topk_sample_one(sequence, k)\n",
    "    if i ==0:\n",
    "      sum = new\n",
    "    else:\n",
    "      sum+=new\n",
    "  return(torch.where(sum>0, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target):\n",
    "  # custom loss function\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  weighted_mse = criterion(output.double(), target.double())\n",
    "  batch_size = output.size()[1]\n",
    "  ssm_err = 0\n",
    "  for i in range(0, batch_size):\n",
    "    SSM1 = SSM(output[:,i,:])\n",
    "    SSM2 = SSM(target[:,i,:])\n",
    "    ssm_err += (torch.sum((SSM1-SSM2)**2)/(SSM2.size(0)**2))\n",
    "\n",
    "\n",
    "  return torch.sum(weighted_mse)+ssm_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stupid-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns batches where piece size is constant within the batch\n",
    "# but piece size is different across batches\n",
    "# and batches are in random order\n",
    "def make_variable_size_batches(data, min_batch_size=10):\n",
    "  # sort data by num beats (element at index 2 in each sublist)\n",
    "  data.sort(key = lambda x: x[2], reverse=False)  # sort descending\n",
    "\n",
    "  # split data into batches, where each batch contains pieces of the same size\n",
    "  batches = []\n",
    "\n",
    "  i = 0  # counter of pieces\n",
    "  \n",
    "  while i < len(data):\n",
    "    this_batch = []\n",
    "    pieces_this_batch = 0\n",
    "    current_beats = data[i][2] # num beats in this batch\n",
    "\n",
    "    # for all pieces with this # of beats\n",
    "    while i < len(data) and data[i][2] == current_beats:\n",
    "      # get tensor from row of data, and reshape \n",
    "      just_tensor = data[i][0].view(1, data[i][0].shape[0], 128)  \n",
    "      this_batch.append(just_tensor)\n",
    "\n",
    "      # increment counters\n",
    "      i += 1\n",
    "      pieces_this_batch += 1\n",
    "\n",
    "    # print(\"this batch\", this_batch)\n",
    "    # print(\"shapes in batch\")\n",
    "    # for p in this_batch:\n",
    "      # print(\"\\t\", p.shape)\n",
    "        \n",
    "    # only save large enough batches\n",
    "    if pieces_this_batch >= min_batch_size:\n",
    "        # reformat pieces in this batch into one tensor of size [batch size, beats, 128]\n",
    "        batch = torch.cat(this_batch, dim=0)\n",
    "\n",
    "        # store batch\n",
    "        batches.append(batch)\n",
    "\n",
    "    # clean up variables\n",
    "    del this_batch\n",
    "    del pieces_this_batch\n",
    "    del current_beats\n",
    "\n",
    "  # randomize batches order\n",
    "  random.shuffle(batches)\n",
    "\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worthy-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_in_directory(filepath):\n",
    "    # get the names of all files, sorted by epoch\n",
    "    files = os.listdir(my_drive_path + filepath)\n",
    "    files_by_epoch = [(name, int(name.split(\"-\")[2])) for name in files]  # parse out epoch\n",
    "    files_by_epoch.sort(key = lambda x: x[1])  # sort by epoch\n",
    "\n",
    "    # load model at each epoch\n",
    "    files_to_load = [my_drive_path + filepath + filename[0] for filename in files_by_epoch]  # paths to each file\n",
    "    # print(\"loading:\\n\", files_to_load, sep=\"\")\n",
    "    models = [torch.load(filename) for filename in files_to_load]  # load models\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "early-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_att = get_models_in_directory(\"trained/attention_model_v2/\")\n",
    "models_lstm = get_models_in_directory(\"trained/lstm_v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "latter-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " )]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "herbal-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " music_generator(\n",
       "   (lstm): LSTM(128, 128)\n",
       "   (attention): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (softmax): Sparsemax(dim=1)\n",
       "   (sigmoid): Sigmoid()\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "amazing-contact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdbd1083c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load validation data\n",
    "val_data = torch.load(my_drive_path + \"usable_data/validation_tempo_all_w_beats_30.csv\")\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secondary-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_pieces_revised(trained_model, initial_vectors, n_pieces, length, batched_ssm):\n",
    "    # generates a batch of n new pieces of music\n",
    "\n",
    "    # freeze generator so it doesn't train anymore\n",
    "    trained_model.eval()  \n",
    "    # start generator on random hidden states and cell states\n",
    "    trained_model.set_random_hidden(n_pieces)\n",
    "  \n",
    "    # initial vectors in format [batch_size, num_notes=10, 128]\n",
    "    # change sequence to [10, batch_size, 128]\n",
    "    sequence = initial_vectors.transpose(0,1)\n",
    "    print(\"sequence\", sequence.shape)\n",
    "    print(\"batched_ssm\", batched_ssm.shape)\n",
    "    max_notes = batched_ssm.shape[0]  # can't generate more notes than the ssm\n",
    "    \n",
    "    # generate [length] more beats for the piece\n",
    "    for i in range(0, min(length, max_notes)):  # one at a time\n",
    "      with torch.no_grad():\n",
    "        # use n_pieces to generate as the batch size\n",
    "        output, _ = trained_model.forward(sequence.float(), n_pieces, sequence, batched_ssm)\n",
    "        next_element = topk_batch_sample(output, 1)\n",
    "      # add element to sequence\n",
    "      sequence = torch.vstack((sequence, next_element.to(\"cpu\")))\n",
    "\n",
    "    # return sequence of beats\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "written-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_val_pieces(trained_model, val_batch, starter_notes=10):\n",
    "    # how many pieces in batch\n",
    "    n = val_batch.shape[0]\n",
    "    \n",
    "    # length of the pieces in this batch\n",
    "    length = val_batch.shape[1]\n",
    "    \n",
    "    # take first 10 notes in format [1, 10, 128]\n",
    "    first_vecs = val_batch[:,0:starter_notes,:]\n",
    "\n",
    "    # create batched SSMs for each piece\n",
    "    batched_ssms = batch_SSM(val_batch.transpose(0,1), n)\n",
    "\n",
    "    # generate pieces\n",
    "    generated = generate_n_pieces_revised(trained_model, first_vecs, n, length - starter_notes, batched_ssms)\n",
    "    \n",
    "    # get loss\n",
    "    single_loss = custom_loss(generated[starter_notes:,:,:], val_batch.transpose(0,1)[starter_notes:,:,:])\n",
    "    single_loss.backward()\n",
    "    loss = single_loss.detach()\n",
    "    \n",
    "    print(\"loss:\", loss)\n",
    "\n",
    "    # clean up variables\n",
    "    del pieces\n",
    "    del first_vecs\n",
    "    del batched_ssms\n",
    "\n",
    "    # return pieces and loss\n",
    "    return generated, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "significant-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_on_data(trained_model, data):\n",
    "    \"\"\"Get loss when generating on given data, generating with a given model\"\"\"\n",
    "    # make batches from val_data\n",
    "    # so we can generate for all pieces of the same length at once\n",
    "    batches = make_variable_size_batches(data, min_batch_size=0)\n",
    "    \n",
    "    # accumulate loss for each batch\n",
    "    cum_loss = 0\n",
    "    for batch_num in tqdm(range(len(batches))):\n",
    "        val_batch = batches[batch_num]\n",
    "        # create pieces to get loss\n",
    "        generated, loss = generate_val_pieces(trained_model, val_batch)\n",
    "        cum_loss += loss\n",
    "        del batch\n",
    "        del loss\n",
    "        del generated\n",
    "        \n",
    "    return cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "serial-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, get validation loss\n",
    "def val_loss_for_all_models(models, val_data):\n",
    "    loss_list = []\n",
    "    for model in models:\n",
    "        loss_list.append(get_loss_on_data(model, val_data))\n",
    "        print(loss_list)\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "marine-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_loss_on_data(models_att[0], val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sought-consistency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/78 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence torch.Size([10, 2, 128])\n",
      "batched_ssm torch.Size([6660, 3330])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-25b6b42142bc>:79: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1666647174771/work/aten/src/ATen/native/TensorShape.cpp:3281.)\n",
      "  weighted = (prev_sequence.T*weights).T # replace .T - see which dimensions we're switching\n",
      "  0%|          | 0/78 [03:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-594c3714ef7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_list_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_for_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list_att\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d3f7f104b00f>\u001b[0m in \u001b[0;36mval_loss_for_all_models\u001b[0;34m(models, val_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_loss_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e88646169fe7>\u001b[0m in \u001b[0;36mget_loss_on_data\u001b[0;34m(trained_model, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mval_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# create pieces to get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_val_pieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f53c189a3a5d>\u001b[0m in \u001b[0;36mgenerate_val_pieces\u001b[0;34m(trained_model, val_batch, starter_notes)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msingle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstarter_notes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstarter_notes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msingle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/csc294/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/envs/csc294/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss_list_att = val_loss_for_all_models(models_att, val_data)\n",
    "print(loss_list_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_lstm = val_loss_for_all_models(models_lstm, val_data)\n",
    "print(loss_list_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-update",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
